{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Building an Event Processing System\n",
    "\n",
    "This tutorial walks through building a production-ready task execution system using **Processor/Executor** patterns.\n",
    "\n",
    "**What You'll Build**: A distributed task executor with:\n",
    "- Priority-based task scheduling\n",
    "- Rate limiting and quota management\n",
    "- Failure handling with retry logic\n",
    "- Monitoring and observability\n",
    "\n",
    "**Prerequisites**: Familiarity with async Python and lionherd-core basics (Event, Flow, Pile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Problem**: You need to execute thousands of API calls with:\n",
    "- Rate limits (100 requests per minute)\n",
    "- Priority (critical tasks first)\n",
    "- Retry logic (transient failures)\n",
    "- Observability (track status in real-time)\n",
    "\n",
    "**Solution**: Processor/Executor pattern provides:\n",
    "- **Executor**: State management with Flow (O(1) status queries)\n",
    "- **Processor**: Background processing with priority queue\n",
    "- **Permission Checks**: Custom rate limiting and quota enforcement\n",
    "- **Flow Progressions**: 1:1 mapping with EventStatus for observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Any, ClassVar\n",
    "\n",
    "from lionherd_core.base import Event, EventStatus, Executor, Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define Your Events\n",
    "\n",
    "Create custom Event subclass for your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APICallEvent(Event):\n",
    "    \"\"\"Event representing an API call.\"\"\"\n",
    "\n",
    "    endpoint: str  # API endpoint to call\n",
    "    method: str = \"GET\"  # HTTP method\n",
    "    payload: dict[str, Any] | None = None  # Request payload\n",
    "    priority_level: str = \"normal\"  # Priority: critical, high, normal, low\n",
    "\n",
    "    async def _invoke(self):\n",
    "        \"\"\"Simulate API call.\"\"\"\n",
    "        # In production: use httpx or aiohttp\n",
    "        await asyncio.sleep(0.1)  # Simulate network latency\n",
    "\n",
    "        # Simulate occasional failures\n",
    "        if \"fail\" in self.endpoint:\n",
    "            raise Exception(f\"API error: {self.endpoint}\")\n",
    "\n",
    "        return {\"status\": 200, \"endpoint\": self.endpoint, \"method\": self.method}\n",
    "\n",
    "\n",
    "# Test event\n",
    "event = APICallEvent(endpoint=\"/users/123\", method=\"GET\", priority_level=\"high\")\n",
    "result = await event.invoke()\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Status: {event.status}\")\n",
    "print(f\"Duration: {event.execution.duration:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Your Processor\n",
    "\n",
    "Define Processor subclass to handle your events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIProcessor(Processor):\n",
    "    \"\"\"Processor for API call events.\"\"\"\n",
    "\n",
    "    event_type: ClassVar[type[Event]] = APICallEvent\n",
    "\n",
    "    # No custom logic yet - basic processor\n",
    "\n",
    "\n",
    "# Processor will be created by Executor\n",
    "print(f\"Processor handles: {APIProcessor.event_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Your Executor\n",
    "\n",
    "Executor manages Flow-based state and processor lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIExecutor(Executor):\n",
    "    \"\"\"Executor for API calls with Flow-based state tracking.\"\"\"\n",
    "\n",
    "    processor_type: ClassVar[type[Processor]] = APIProcessor\n",
    "\n",
    "\n",
    "# Create executor with processor config\n",
    "executor = APIExecutor(\n",
    "    processor_config={\n",
    "        \"queue_capacity\": 10,  # Process 10 events per batch\n",
    "        \"capacity_refresh_time\": 1.0,  # 1 second between batches\n",
    "        \"concurrency_limit\": 5,  # Max 5 concurrent API calls\n",
    "    },\n",
    "    name=\"api_executor\",\n",
    ")\n",
    "\n",
    "print(f\"Executor: {executor}\")\n",
    "print(f\"Progressions (status tracking): {[p.name for p in executor.states.progressions]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Add and Process Events\n",
    "\n",
    "Add events to executor and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API call events\n",
    "endpoints = [\n",
    "    \"/users/1\",\n",
    "    \"/posts/42\",\n",
    "    \"/comments/99\",\n",
    "    \"/users/2\",\n",
    "    \"/posts/43\",\n",
    "]\n",
    "\n",
    "for endpoint in endpoints:\n",
    "    event = APICallEvent(endpoint=endpoint, method=\"GET\")\n",
    "    # No priority specified - defaults to created_at timestamp\n",
    "    await executor.append(event)\n",
    "\n",
    "print(f\"Executor after adding events: {executor}\")\n",
    "print(f\"Pending: {len(executor.pending_events)}\")\n",
    "\n",
    "# Start executor and process\n",
    "await executor.start()\n",
    "await executor.forward()  # Process one batch\n",
    "await asyncio.sleep(0.6)  # Wait for completion\n",
    "\n",
    "print(\"\\nAfter processing:\")\n",
    "print(f\"  Pending: {len(executor.pending_events)}\")\n",
    "print(f\"  Completed: {len(executor.completed_events)}\")\n",
    "print(f\"  Failed: {len(executor.failed_events)}\")\n",
    "\n",
    "# Inspect completed events\n",
    "for event in executor.completed_events:\n",
    "    print(f\"  ✓ {event.endpoint}: {event.execution.response['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Add Rate Limiting\n",
    "\n",
    "Implement rate limiting with custom `request_permission()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimitedAPIProcessor(APIProcessor):\n",
    "    \"\"\"API processor with rate limiting.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.request_timestamps: list[float] = []\n",
    "        self.rate_limit = 3  # Max 3 requests per second\n",
    "        self.time_window = 1.0  # 1 second window\n",
    "\n",
    "    async def request_permission(self, **kwargs: Any) -> bool:\n",
    "        \"\"\"Check rate limit before processing.\"\"\"\n",
    "        now = time.time()\n",
    "\n",
    "        # Remove timestamps outside time window\n",
    "        self.request_timestamps = [\n",
    "            ts for ts in self.request_timestamps if now - ts < self.time_window\n",
    "        ]\n",
    "\n",
    "        # Check if under rate limit\n",
    "        if len(self.request_timestamps) >= self.rate_limit:\n",
    "            print(f\"Rate limit exceeded: {len(self.request_timestamps)}/{self.rate_limit}\")\n",
    "            return False\n",
    "\n",
    "        # Grant permission and record timestamp\n",
    "        self.request_timestamps.append(now)\n",
    "        return True\n",
    "\n",
    "\n",
    "class RateLimitedExecutor(Executor):\n",
    "    processor_type: ClassVar[type[Processor]] = RateLimitedAPIProcessor\n",
    "\n",
    "\n",
    "# Test rate limiting\n",
    "rate_executor = RateLimitedExecutor(\n",
    "    processor_config={\n",
    "        \"queue_capacity\": 10,\n",
    "        \"capacity_refresh_time\": 0.5,\n",
    "    },\n",
    "    name=\"rate_limited_api\",\n",
    ")\n",
    "\n",
    "# Add 6 events (rate limit: 3/sec)\n",
    "for i in range(6):\n",
    "    await rate_executor.append(APICallEvent(endpoint=f\"/data/{i}\"))\n",
    "\n",
    "await rate_executor.start()\n",
    "print(\"Processing with rate limit (3/sec):\")\n",
    "await rate_executor.forward()\n",
    "await asyncio.sleep(0.4)\n",
    "\n",
    "print(f\"\\nFirst batch: {rate_executor.status_counts()}\")\n",
    "print(\"(3 completed, 3 pending due to rate limit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Handle Failures\n",
    "\n",
    "Events that fail are tracked in `failed_events` progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create events with some failures\n",
    "failure_executor = APIExecutor(\n",
    "    processor_config={\"queue_capacity\": 10, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "endpoints_with_failures = [\n",
    "    \"/users/1\",  # Success\n",
    "    \"/fail/endpoint\",  # Fail (contains 'fail')\n",
    "    \"/posts/42\",  # Success\n",
    "    \"/fail/another\",  # Fail\n",
    "    \"/comments/99\",  # Success\n",
    "]\n",
    "\n",
    "for endpoint in endpoints_with_failures:\n",
    "    await failure_executor.append(APICallEvent(endpoint=endpoint))\n",
    "\n",
    "await failure_executor.start()\n",
    "await failure_executor.forward()\n",
    "await asyncio.sleep(0.6)\n",
    "\n",
    "print(\"Processing results:\")\n",
    "print(f\"  Completed: {len(failure_executor.completed_events)}\")\n",
    "print(f\"  Failed: {len(failure_executor.failed_events)}\")\n",
    "\n",
    "# Inspect failures\n",
    "print(\"\\nFailed events:\")\n",
    "for event in failure_executor.failed_events:\n",
    "    print(f\"  ✗ {event.endpoint}: {event.execution.error}\")\n",
    "    print(f\"    Retryable: {event.execution.retryable}\")\n",
    "\n",
    "# Retry failed events\n",
    "print(\"\\nRetrying failed events...\")\n",
    "for failed_event in failure_executor.failed_events:\n",
    "    if failed_event.execution.retryable:\n",
    "        # Create fresh event for retry\n",
    "        retry_event = failed_event.as_fresh_event()\n",
    "        # Note: Will still fail if endpoint contains 'fail'\n",
    "        print(f\"  Retry queued: {retry_event.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Priority Processing\n",
    "\n",
    "Process high-priority events first with custom priority values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority mapping\n",
    "PRIORITY_MAP = {\n",
    "    \"critical\": 1.0,\n",
    "    \"high\": 5.0,\n",
    "    \"normal\": 10.0,\n",
    "    \"low\": 20.0,\n",
    "}\n",
    "\n",
    "priority_executor = APIExecutor(\n",
    "    processor_config={\"queue_capacity\": 10, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "# Add events with different priorities\n",
    "tasks = [\n",
    "    (\"low\", \"/logs/upload\"),\n",
    "    (\"critical\", \"/alerts/fire\"),\n",
    "    (\"normal\", \"/data/sync\"),\n",
    "    (\"high\", \"/payment/process\"),\n",
    "    (\"low\", \"/cache/cleanup\"),\n",
    "]\n",
    "\n",
    "for priority_level, endpoint in tasks:\n",
    "    event = APICallEvent(endpoint=endpoint, priority_level=priority_level)\n",
    "    priority = PRIORITY_MAP[priority_level]\n",
    "    await priority_executor.append(event, priority=priority)\n",
    "    print(f\"Queued: {endpoint} (priority={priority})\")\n",
    "\n",
    "await priority_executor.start()\n",
    "await priority_executor.forward()\n",
    "await asyncio.sleep(0.6)\n",
    "\n",
    "print(\"\\nProcessing order (by priority):\")\n",
    "for event in priority_executor.completed_events:\n",
    "    print(f\"  {event.priority_level:8s} - {event.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Monitoring and Debugging\n",
    "\n",
    "Use Flow progressions for real-time monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monitoring executor\n",
    "monitor_executor = APIExecutor(\n",
    "    processor_config={\n",
    "        \"queue_capacity\": 5,\n",
    "        \"capacity_refresh_time\": 0.3,\n",
    "        \"concurrency_limit\": 2,\n",
    "    },\n",
    "    name=\"monitored_api\",\n",
    ")\n",
    "\n",
    "# Add events\n",
    "for i in range(8):\n",
    "    await monitor_executor.append(APICallEvent(endpoint=f\"/task/{i}\"))\n",
    "\n",
    "await monitor_executor.start()\n",
    "\n",
    "# Monitor during processing\n",
    "print(\"Real-time monitoring:\")\n",
    "for iteration in range(3):\n",
    "    await monitor_executor.forward()\n",
    "    await asyncio.sleep(0.15)  # Partial wait\n",
    "\n",
    "    counts = monitor_executor.status_counts()\n",
    "    print(f\"\\nIteration {iteration + 1}:\")\n",
    "    print(f\"  {monitor_executor.inspect_state()}\")\n",
    "\n",
    "    await asyncio.sleep(0.2)  # Wait for completion\n",
    "\n",
    "# Final state\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Final state:\")\n",
    "print(f\"  Total events: {len(monitor_executor.states.items)}\")\n",
    "print(f\"  Completed: {len(monitor_executor.completed_events)}\")\n",
    "print(\n",
    "    f\"  Average duration: {sum(e.execution.duration for e in monitor_executor.completed_events) / len(monitor_executor.completed_events):.4f}s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Production Patterns\n",
    "\n",
    "Best practices for production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Cleanup completed events\n",
    "cleanup_executor = APIExecutor(processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5})\n",
    "\n",
    "for i in range(10):\n",
    "    await cleanup_executor.append(APICallEvent(endpoint=f\"/task/{i}\"))\n",
    "\n",
    "await cleanup_executor.start()\n",
    "await cleanup_executor.forward()\n",
    "await asyncio.sleep(0.6)\n",
    "\n",
    "print(f\"Before cleanup: {cleanup_executor}\")\n",
    "\n",
    "# Clean up completed/failed events (free memory)\n",
    "removed = await cleanup_executor.cleanup_events([EventStatus.COMPLETED, EventStatus.FAILED])\n",
    "print(f\"Removed {removed} events\")\n",
    "print(f\"After cleanup: {cleanup_executor}\")\n",
    "\n",
    "# Pattern 2: State persistence\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"State persistence:\")\n",
    "state_data = cleanup_executor.states.to_dict()\n",
    "print(\n",
    "    f\"Serialized state: {len(state_data['items'])} items, {len(state_data['progressions'])} progressions\"\n",
    ")\n",
    "\n",
    "# Pattern 3: Graceful shutdown\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Graceful shutdown:\")\n",
    "shutdown_executor = APIExecutor(\n",
    "    processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    await shutdown_executor.append(APICallEvent(endpoint=f\"/shutdown/{i}\"))\n",
    "\n",
    "# Must call start() to create processor\n",
    "await shutdown_executor.start()\n",
    "\n",
    "# Start background processing\n",
    "exec_task = asyncio.create_task(shutdown_executor.processor.execute())\n",
    "\n",
    "await asyncio.sleep(0.3)  # Let some process\n",
    "\n",
    "# Graceful stop\n",
    "await shutdown_executor.stop()\n",
    "print(f\"Stopped: {shutdown_executor.processor.is_stopped()}\")\n",
    "await exec_task  # Wait for cleanup\n",
    "\n",
    "print(f\"Final state: {shutdown_executor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**What You Built**:\n",
    "- ✅ Custom Event subclass (APICallEvent)\n",
    "- ✅ Processor with rate limiting (request_permission)\n",
    "- ✅ Executor with Flow-based state tracking\n",
    "- ✅ Priority-based scheduling\n",
    "- ✅ Failure handling and retry logic\n",
    "- ✅ Real-time monitoring\n",
    "- ✅ Production patterns (cleanup, persistence, shutdown)\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. **Flow Progressions**: 1:1 mapping with EventStatus enables O(1) queries\n",
    "2. **Permission Checks**: Override `request_permission()` for custom gating\n",
    "3. **Priority Queue**: Lower values processed first (customizable per event)\n",
    "4. **Concurrency Control**: Semaphore limits concurrent executions\n",
    "5. **State Management**: Serialize Flow for persistence and recovery\n",
    "\n",
    "**Next Steps**:\n",
    "- See `notebooks/tutorials/flow_state_tracking.ipynb` for deep dive on Flow progressions\n",
    "- See `notebooks/references/processor_executor.ipynb` for complete API reference\n",
    "- Check API docs: `docs/api/processor.md`, `docs/api/executor.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
