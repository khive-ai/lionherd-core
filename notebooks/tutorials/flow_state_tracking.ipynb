{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Flow-Based State Tracking\n",
    "\n",
    "This tutorial explores the **Flow-based state management** pattern used by Executor.\n",
    "\n",
    "**Core Concept**: Map EventStatus enum 1:1 to Flow progressions for O(1) status queries.\n",
    "\n",
    "**Why This Matters**:\n",
    "- Traditional approach: Linear scan through all events (O(n))\n",
    "- Flow approach: Direct progression lookup (O(1))\n",
    "- Enables real-time monitoring at scale (10k+ events)\n",
    "\n",
    "**Prerequisites**: Understanding of Flow, Progression, Event basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Problem**: Track status of 10,000 events in real-time.\n",
    "\n",
    "**Naive Solution** (O(n)):\n",
    "```python\n",
    "completed = [e for e in events if e.status == EventStatus.COMPLETED]  # Scan all\n",
    "```\n",
    "\n",
    "**Flow Solution** (O(1)):\n",
    "```python\n",
    "completed_ids = flow.get_progression(\"completed\").order  # Direct access\n",
    "completed = [flow.items[id] for id in completed_ids]  # O(1) per item\n",
    "```\n",
    "\n",
    "**Key Insight**: Flow progressions act as **pre-computed indices** for event status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import asyncio\n",
    "from typing import ClassVar\n",
    "\n",
    "from lionherd_core.base import Event, EventStatus, Executor, Flow, Processor, Progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Flow Basics\n",
    "\n",
    "Flow combines two Pile instances: `items` and `progressions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple events\n",
    "class SimpleEvent(Event):\n",
    "    name: str = \"event\"\n",
    "\n",
    "    async def _invoke(self):\n",
    "        return \"done\"\n",
    "\n",
    "\n",
    "# Create Flow\n",
    "flow = Flow[Event, Progression](item_type=Event, name=\"event_flow\")\n",
    "\n",
    "print(\"Flow structure:\")\n",
    "print(f\"  items: {type(flow.items)} (stores events)\")\n",
    "print(f\"  progressions: {type(flow.progressions)} (stores ordered UUIDs)\")\n",
    "\n",
    "# Add events to items pile\n",
    "events = [SimpleEvent(name=f\"event_{i}\") for i in range(3)]\n",
    "for event in events:\n",
    "    flow.add_item(event)\n",
    "\n",
    "print(\"\\nFlow contents:\")\n",
    "print(f\"  Items in pile: {len(flow.items)}\")\n",
    "print(f\"  Progressions: {len(flow.progressions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: EventStatus Enum\n",
    "\n",
    "EventStatus defines all possible event states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect EventStatus enum\n",
    "print(\"EventStatus values:\")\n",
    "for status in EventStatus:\n",
    "    print(f\"  {status.value}\")\n",
    "\n",
    "# Event starts as PENDING\n",
    "event = SimpleEvent(name=\"test\")\n",
    "print(f\"\\nNew event status: {event.status} ({event.status.value})\")\n",
    "\n",
    "# Status changes during lifecycle\n",
    "result = await event.invoke()\n",
    "print(f\"After invoke: {event.status} ({event.status.value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 1:1 Mapping Architecture\n",
    "\n",
    "Executor creates one progression per EventStatus value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProcessor(Processor):\n",
    "    event_type: ClassVar[type[Event]] = SimpleEvent\n",
    "\n",
    "\n",
    "class SimpleExecutor(Executor):\n",
    "    processor_type: ClassVar[type[Processor]] = SimpleProcessor\n",
    "\n",
    "\n",
    "# Create executor\n",
    "executor = SimpleExecutor(processor_config={\"queue_capacity\": 10, \"capacity_refresh_time\": 0.5})\n",
    "\n",
    "# Inspect Flow progressions\n",
    "print(\"Executor Flow progressions:\")\n",
    "for prog in executor.states.progressions:\n",
    "    print(f\"  {prog.name}: {len(prog)} events\")\n",
    "\n",
    "# Verify 1:1 mapping\n",
    "print(f\"\\nEventStatus count: {len(list(EventStatus))}\")\n",
    "print(f\"Progression count: {len(executor.states.progressions)}\")\n",
    "print(f\"1:1 mapping: {len(list(EventStatus)) == len(executor.states.progressions)}\")\n",
    "\n",
    "# Progression names match EventStatus values\n",
    "status_values = {s.value for s in EventStatus}\n",
    "prog_names = {p.name for p in executor.states.progressions}\n",
    "print(f\"Names match: {status_values == prog_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Progression Updates\n",
    "\n",
    "When event status changes, Executor updates progressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add events to executor\n",
    "test_executor = SimpleExecutor(processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5})\n",
    "\n",
    "event1 = SimpleEvent(name=\"event_1\")\n",
    "event2 = SimpleEvent(name=\"event_2\")\n",
    "\n",
    "await test_executor.append(event1)\n",
    "await test_executor.append(event2)\n",
    "\n",
    "print(\"After append:\")\n",
    "print(f\"  Event status: {event1.status}\")\n",
    "print(f\"  Pending progression: {len(test_executor.states.get_progression('pending'))} events\")\n",
    "print(f\"  Event in pending: {event1.id in test_executor.states.get_progression('pending')}\")\n",
    "\n",
    "# Process events\n",
    "await test_executor.start()\n",
    "await test_executor.forward()\n",
    "await asyncio.sleep(0.05)\n",
    "\n",
    "print(\"\\nAfter processing:\")\n",
    "print(f\"  Event status: {event1.status}\")\n",
    "print(f\"  Completed progression: {len(test_executor.states.get_progression('completed'))} events\")\n",
    "print(f\"  Event in completed: {event1.id in test_executor.states.get_progression('completed')}\")\n",
    "print(f\"  Event in pending: {event1.id in test_executor.states.get_progression('pending')}\")\n",
    "\n",
    "# Invariant: Event exists in exactly ONE progression\n",
    "count = sum(1 for prog in test_executor.states.progressions if event1.id in prog)\n",
    "print(f\"\\nEvent in {count} progression(s) (should be 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Multi-Progression Use Cases\n",
    "\n",
    "While Executor enforces single-progression, Flow supports M:N relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Flow with multiple progressions\n",
    "custom_flow = Flow[Event, Progression](name=\"multi_prog\")\n",
    "\n",
    "# Add progressions for different views\n",
    "custom_flow.add_progression(Progression(name=\"high_priority\"))\n",
    "custom_flow.add_progression(Progression(name=\"api_calls\"))\n",
    "custom_flow.add_progression(Progression(name=\"team_a\"))\n",
    "\n",
    "# Event can be in multiple progressions (cross-cutting concerns)\n",
    "event = SimpleEvent(name=\"urgent_api_call\")\n",
    "custom_flow.add_item(event, progressions=[\"high_priority\", \"api_calls\", \"team_a\"])\n",
    "\n",
    "print(\"Multi-progression membership:\")\n",
    "for prog in custom_flow.progressions:\n",
    "    if event.id in prog:\n",
    "        print(f\"  ✓ {prog.name}\")\n",
    "\n",
    "print(\"\\nUse cases for M:N:\")\n",
    "print(\"  - Cross-cutting concerns (priority, team, type)\")\n",
    "print(\"  - Multiple views of same data\")\n",
    "print(\"  - Tag-based organization\")\n",
    "\n",
    "print(\"\\nExecutor's single-progression invariant:\")\n",
    "print(\"  - Enforces mutually exclusive status\")\n",
    "print(\"  - Event cannot be both PENDING and COMPLETED\")\n",
    "print(\"  - Simplifies state machine logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Querying Patterns\n",
    "\n",
    "Common patterns for querying events by status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executor with events\n",
    "query_executor = SimpleExecutor(\n",
    "    processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "# Add 10 events\n",
    "for i in range(10):\n",
    "    await query_executor.append(SimpleEvent(name=f\"task_{i}\"))\n",
    "\n",
    "await query_executor.start()\n",
    "await query_executor.forward()\n",
    "await asyncio.sleep(0.05)\n",
    "\n",
    "# Pattern 1: Get all events of a status\n",
    "completed = query_executor.get_events_by_status(EventStatus.COMPLETED)\n",
    "print(f\"Pattern 1 - All completed: {len(completed)} events\")\n",
    "\n",
    "# Pattern 2: Count events per status\n",
    "counts = query_executor.status_counts()\n",
    "print(f\"\\nPattern 2 - Status counts: {counts}\")\n",
    "\n",
    "# Pattern 3: Direct progression access\n",
    "pending_prog = query_executor.states.get_progression(\"pending\")\n",
    "pending_events = [query_executor.states.items[uid] for uid in pending_prog.order]\n",
    "print(f\"\\nPattern 3 - Pending events: {len(pending_events)}\")\n",
    "\n",
    "# Pattern 4: Filter by predicate\n",
    "completed_events = query_executor.completed_events\n",
    "fast_events = [e for e in completed_events if e.execution.duration < 0.01]\n",
    "print(f\"\\nPattern 4 - Fast completed: {len(fast_events)} events\")\n",
    "\n",
    "# Pattern 5: Check membership\n",
    "event = completed_events[0] if completed_events else None\n",
    "if event:\n",
    "    in_completed = event.id in query_executor.states.get_progression(\"completed\")\n",
    "    print(f\"\\nPattern 5 - Event in completed: {in_completed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Performance Analysis\n",
    "\n",
    "Compare O(n) scan vs O(1) progression lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create executor with many events\n",
    "perf_executor = SimpleExecutor(\n",
    "    processor_config={\"queue_capacity\": 100, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "# Add 1000 events\n",
    "print(\"Adding 1000 events...\")\n",
    "for i in range(1000):\n",
    "    await perf_executor.append(SimpleEvent(name=f\"event_{i}\"))\n",
    "\n",
    "await perf_executor.start()\n",
    "await perf_executor.forward()\n",
    "await asyncio.sleep(2.0)  # Let them process\n",
    "\n",
    "print(f\"Status: {perf_executor.status_counts()}\")\n",
    "\n",
    "# Benchmark 1: O(n) scan (naive approach)\n",
    "all_events = list(perf_executor.states.items)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):  # 100 iterations\n",
    "    completed_naive = [e for e in all_events if e.status == EventStatus.COMPLETED]\n",
    "scan_time = (time.perf_counter() - start) * 1000  # ms\n",
    "\n",
    "print(f\"\\nO(n) scan (100 iterations): {scan_time:.2f}ms\")\n",
    "print(f\"  Per query: {scan_time / 100:.3f}ms\")\n",
    "\n",
    "# Benchmark 2: O(1) progression lookup\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):  # 100 iterations\n",
    "    completed_flow = perf_executor.get_events_by_status(EventStatus.COMPLETED)\n",
    "flow_time = (time.perf_counter() - start) * 1000  # ms\n",
    "\n",
    "print(f\"\\nO(1) progression (100 iterations): {flow_time:.2f}ms\")\n",
    "print(f\"  Per query: {flow_time / 100:.3f}ms\")\n",
    "\n",
    "# Speedup\n",
    "speedup = scan_time / flow_time if flow_time > 0 else float(\"inf\")\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster\")\n",
    "print(f\"Savings: {scan_time - flow_time:.2f}ms per 100 queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Serialization and Audit\n",
    "\n",
    "Serialize Flow state for persistence and audit trails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executor with events\n",
    "audit_executor = SimpleExecutor(\n",
    "    processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5},\n",
    "    name=\"audit_exec\",\n",
    ")\n",
    "\n",
    "# Add and process events\n",
    "for i in range(8):\n",
    "    await audit_executor.append(SimpleEvent(name=f\"audit_event_{i}\"))\n",
    "\n",
    "await audit_executor.start()\n",
    "await audit_executor.forward()\n",
    "await asyncio.sleep(0.05)\n",
    "\n",
    "print(f\"Executor state: {audit_executor}\")\n",
    "\n",
    "# Serialize Flow state\n",
    "state_snapshot = audit_executor.states.to_dict()\n",
    "\n",
    "print(\"\\nSerialized snapshot:\")\n",
    "print(f\"  Flow name: {state_snapshot['name']}\")\n",
    "print(f\"  Items: {len(state_snapshot['items'])} events\")\n",
    "print(f\"  Progressions: {len(state_snapshot['progressions'])}\")\n",
    "\n",
    "# Show status counts\n",
    "print(\"\\nStatus distribution:\")\n",
    "for status in EventStatus:\n",
    "    count = len(audit_executor.get_events_by_status(status))\n",
    "    if count > 0:\n",
    "        print(f\"    {status.value}: {count} events\")\n",
    "\n",
    "# Serialization format verification\n",
    "print(\"\\nSerialization format:\")\n",
    "print(f\"  Snapshot keys: {list(state_snapshot.keys())}\")\n",
    "print(\"  Can be saved to JSON/disk: ✓\")\n",
    "print(\"  Can be transmitted over network: ✓\")\n",
    "print(\"  Can be used for audit trails: ✓\")\n",
    "\n",
    "# Note: Restore would use Flow.from_dict(state_snapshot)\n",
    "# But requires all events to have complete execution state\n",
    "print(\"\\nFor production use:\")\n",
    "print(\"  - Save state_snapshot to database/file\")\n",
    "print(\"  - Restore with Flow.from_dict(snapshot)\")\n",
    "print(\"  - Use for crash recovery and audit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced Patterns\n",
    "\n",
    "Production patterns for Flow-based state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: State transitions with logging\n",
    "class LoggingExecutor(SimpleExecutor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.transition_log = []\n",
    "\n",
    "    async def _update_progression(self, event, force_status=None):\n",
    "        \"\"\"Override to log transitions.\"\"\"\n",
    "        old_status = event.status\n",
    "        await super()._update_progression(event, force_status)\n",
    "        new_status = force_status if force_status else event.execution.status\n",
    "\n",
    "        if old_status != new_status:\n",
    "            self.transition_log.append(\n",
    "                {\n",
    "                    \"event_id\": str(event.id),\n",
    "                    \"from\": old_status.value,\n",
    "                    \"to\": new_status.value,\n",
    "                    \"timestamp\": time.time(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "logging_executor = LoggingExecutor(\n",
    "    processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    await logging_executor.append(SimpleEvent(name=f\"logged_{i}\"))\n",
    "\n",
    "await logging_executor.start()\n",
    "await logging_executor.forward()\n",
    "await asyncio.sleep(0.05)\n",
    "\n",
    "print(\"Pattern 1 - Transition log:\")\n",
    "for transition in logging_executor.transition_log:\n",
    "    print(f\"  {transition['from']} → {transition['to']}\")\n",
    "\n",
    "# Pattern 2: Conditional cleanup\n",
    "print(\"\\nPattern 2 - Conditional cleanup:\")\n",
    "cleanup_executor = SimpleExecutor(\n",
    "    processor_config={\"queue_capacity\": 5, \"capacity_refresh_time\": 0.5}\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    await cleanup_executor.append(SimpleEvent(name=f\"cleanup_{i}\"))\n",
    "\n",
    "await cleanup_executor.start()\n",
    "await cleanup_executor.forward()\n",
    "await asyncio.sleep(0.05)\n",
    "\n",
    "print(f\"Before cleanup: {cleanup_executor}\")\n",
    "\n",
    "# Clean up only completed (keep failed for retry)\n",
    "removed = await cleanup_executor.cleanup_events([EventStatus.COMPLETED])\n",
    "print(f\"Cleaned up {removed} completed events\")\n",
    "print(f\"After cleanup: {cleanup_executor}\")\n",
    "\n",
    "# Pattern 3: Status-based routing\n",
    "print(\"\\nPattern 3 - Status-based routing:\")\n",
    "\n",
    "\n",
    "def route_events(executor):\n",
    "    \"\"\"Route events based on status.\"\"\"\n",
    "    failed = executor.failed_events\n",
    "    completed = executor.completed_events\n",
    "\n",
    "    # Route failed to retry queue\n",
    "    if failed:\n",
    "        print(f\"  Routing {len(failed)} failed events to retry queue\")\n",
    "\n",
    "    # Route completed to archive\n",
    "    if completed:\n",
    "        print(f\"  Routing {len(completed)} completed events to archive\")\n",
    "\n",
    "    return {\"retry\": failed, \"archive\": completed}\n",
    "\n",
    "\n",
    "routes = route_events(cleanup_executor)\n",
    "print(f\"Routed to {len(routes)} destinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Flow-Based State Tracking**:\n",
    "\n",
    "1. **Architecture**: 1:1 mapping between EventStatus enum and Flow progressions\n",
    "2. **Performance**: O(1) status queries via direct progression lookup\n",
    "3. **Invariant**: Events exist in exactly ONE progression at a time\n",
    "4. **Flexibility**: Flow supports M:N for custom use cases\n",
    "5. **Serialization**: Full state capture for persistence and audit\n",
    "6. **Observability**: Real-time monitoring via progression counts\n",
    "\n",
    "**Design Benefits**:\n",
    "- **Scalability**: O(1) queries handle 10k+ events\n",
    "- **Clarity**: Progression names match domain (pending, completed, failed)\n",
    "- **Audit**: Serialize state for compliance and debugging\n",
    "- **Flexibility**: Add custom progressions for cross-cutting concerns\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Use Flow progressions as **pre-computed indices**\n",
    "- Enforce **single-progression invariant** for state machines\n",
    "- Leverage **M:N relationships** for tags and views\n",
    "- Serialize Flow for **crash recovery** and **audit trails**\n",
    "\n",
    "**Next Steps**:\n",
    "- Build event processor: `notebooks/tutorials/event_processing_system.ipynb`\n",
    "- API reference: `notebooks/references/processor_executor.ipynb`\n",
    "- Flow deep dive: `notebooks/references/flow.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
