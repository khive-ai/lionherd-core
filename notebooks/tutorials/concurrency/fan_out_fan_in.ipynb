{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fan-Out/Fan-In Pattern with Queue Workers\n",
    "\n",
    "**Category**: Concurrency\n",
    "**Difficulty**: Advanced\n",
    "**Time**: 25-35 minutes\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In production systems, you often need to distribute work across multiple concurrent workers for parallelism, then collect and aggregate results. For example, processing user uploads (resize images, extract metadata, scan for malware) across a worker pool, or distributing API requests to fetch data from multiple endpoints concurrently. The challenge is coordinating work distribution (fan-out), managing worker lifecycles, and collecting results efficiently (fan-in) while handling errors gracefully.\n",
    "\n",
    "Consider a data pipeline that needs to process 10,000 records from a database. Sequential processing would take hours. You need to distribute records across 10 workers, each processing batches concurrently. But how do you feed work to workers? How do you know when all work is complete? How do you handle workers that crash? How do you collect results without blocking producers?\n",
    "\n",
    "**Why This Matters**:\n",
    "- **Throughput**: Worker pools maximize CPU and I/O utilization (10-100x speedup for parallelizable tasks)\n",
    "- **Reliability**: Proper queue patterns prevent work loss, handle backpressure, and isolate worker failures\n",
    "- **Resource Management**: Bounded queues prevent memory exhaustion from fast producers overwhelming slow consumers\n",
    "\n",
    "**What You'll Build**:\n",
    "A production-ready fan-out/fan-in system using lionherd-core's `Queue` and `create_task_group` that distributes work across multiple concurrent workers, collects results efficiently, and handles errors gracefully with proper lifecycle management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Prior Knowledge**:\n",
    "- Python async/await fundamentals\n",
    "- Producer-consumer pattern basics\n",
    "- Exception handling in concurrent contexts\n",
    "- Task groups and structured concurrency\n",
    "\n",
    "**Required Packages**:\n",
    "```bash\n",
    "pip install lionherd-core  # >=0.1.0\n",
    "```\n",
    "\n",
    "**Optional Reading**:\n",
    "- [API Reference: Concurrency Primitives](../../docs/api/libs/concurrency/primitives.md)\n",
    "- [API Reference: Task Groups](../../docs/api/libs/concurrency/task.md)\n",
    "- [Reference Notebook: Concurrency Primitives](../references/concurrency_primitives.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import asyncio\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Callable, Generic, TypeVar\n",
    "\n",
    "# lionherd-core\n",
    "from lionherd_core.libs.concurrency import (\n",
    "    Event,\n",
    "    Queue,\n",
    "    create_task_group,\n",
    "    current_time,\n",
    "    sleep,\n",
    ")\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "R = TypeVar(\"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "We'll implement a fan-out/fan-in system using queues and worker pools:\n",
    "\n",
    "1. **Work Queue**: Producers add tasks to a shared queue (fan-out)\n",
    "2. **Worker Pool**: N workers concurrently consume and process tasks\n",
    "3. **Result Collection**: Workers send results to a result queue (fan-in)\n",
    "4. **Lifecycle Management**: Coordinate worker startup, shutdown, and completion signaling\n",
    "\n",
    "**Key lionherd-core Components**:\n",
    "- `Queue`: Thread-safe async queue with backpressure (maxsize)\n",
    "- `create_task_group`: Structured concurrency for worker lifecycle management\n",
    "- `Event`: Signal completion when all work is done\n",
    "- `get_nowait`: Non-blocking queue read for graceful shutdown\n",
    "\n",
    "**Flow**:\n",
    "```\n",
    "Producer \u2192 [Work Queue] \u2192 Worker 1 \u2192 [Result Queue] \u2192 Result Collector\n",
    "                       \u2193\u2192 Worker 2 \u2192\u2193\n",
    "                       \u2193\u2192 Worker 3 \u2192\u2193\n",
    "                       \u2193\u2192 Worker N \u2192\u2193\n",
    "```\n",
    "\n",
    "**Expected Outcome**: Process tasks in parallel across worker pool, collect all results, and shut down gracefully when complete."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Quick Start: Worker Pool in 30 Seconds\n",
    "\n",
    "from lionherd_core.libs.concurrency import Queue, create_task_group, sleep\n",
    "import asyncio\n",
    "\n",
    "async def worker(worker_id: int, queue: Queue, results: list):\n",
    "    \"\"\"Pull tasks from queue and process.\"\"\"\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        if item is None:  # Shutdown signal\n",
    "            await queue.put(None)  # Pass to other workers\n",
    "            break\n",
    "        \n",
    "        # Process\n",
    "        await sleep(0.05)\n",
    "        results.append(f\"Worker {worker_id}: {item}\")\n",
    "\n",
    "# Try it:\n",
    "work_queue = Queue.with_maxsize(10)\n",
    "results = []\n",
    "\n",
    "async with create_task_group() as tg:\n",
    "    # Start 3 workers\n",
    "    for i in range(3):\n",
    "        tg.start_soon(worker, i, work_queue, results)\n",
    "    \n",
    "    # Add work\n",
    "    for i in range(10):\n",
    "        await work_queue.put(f\"task-{i}\")\n",
    "    \n",
    "    # Shutdown\n",
    "    await work_queue.put(None)\n",
    "\n",
    "print(f\"Processed {len(results)} items with 3 workers\")\n",
    "for r in results[:5]:\n",
    "    print(f\"  {r}\")\n",
    "\n",
    "# \ud83d\udc47 Now read below to understand production-ready worker pools"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Task and Result Data Structures\n",
    "\n",
    "We need to represent work items (tasks) and their outcomes (results). Tasks carry data and processing logic. Results track success/failure and processing metadata.\n",
    "\n",
    "**Why Structured Data**: Type-safe tasks prevent runtime errors. Result metadata enables debugging, performance analysis, and error recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkItem(Generic[T]):\n",
    "    \"\"\"A unit of work to process.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    data: T\n",
    "    priority: int = 0  # For priority queue variants\n",
    "    metadata: dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"WorkItem(id={self.id!r}, data={self.data!r})\"\n",
    "\n",
    "\n",
    "class ResultStatus(Enum):\n",
    "    \"\"\"Processing outcome.\"\"\"\n",
    "\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkResult(Generic[R]):\n",
    "    \"\"\"Result of processing a work item.\"\"\"\n",
    "\n",
    "    work_id: str\n",
    "    status: ResultStatus\n",
    "    result: R | None = None\n",
    "    error: Exception | None = None\n",
    "    worker_id: int = 0\n",
    "    duration: float = 0.0  # Processing time in seconds\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"WorkResult(id={self.work_id!r}, status={self.status.value}, worker={self.worker_id}, duration={self.duration:.3f}s)\"\n",
    "\n",
    "\n",
    "# Example: Create work item and result\n",
    "work = WorkItem(id=\"task-001\", data={\"value\": 42})\n",
    "result = WorkResult(\n",
    "    work_id=\"task-001\",\n",
    "    status=ResultStatus.SUCCESS,\n",
    "    result=\"processed\",\n",
    "    worker_id=1,\n",
    "    duration=0.15,\n",
    ")\n",
    "\n",
    "print(f\"Work: {work}\")\n",
    "print(f\"Result: {result}\")",
    "\n\n",
    "**Notes**:\n",
    "- **Generic Types**: `WorkItem[T]` and `WorkResult[R]` provide type safety - the compiler catches mismatches\n",
    "- **Worker ID**: Tracking which worker processed each task enables debugging and load balancing analysis\n",
    "- **Duration**: Essential for identifying slow tasks and performance bottlenecks\n",
    "- **Metadata**: Store retry count, timestamps, or custom context without polluting the core structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Basic Worker Pattern\n",
    "\n",
    "A worker continuously pulls tasks from the work queue, processes them, and sends results to the result queue. Workers must handle the \"poison pill\" pattern for graceful shutdown - a sentinel value (None) signals \"no more work.\"\n",
    "\n",
    "**Why Poison Pill**: Without a shutdown signal, workers would block forever on `queue.get()`. The poison pill pattern ensures deterministic shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def worker(\n",
    "    worker_id: int,\n",
    "    work_queue: Queue[WorkItem[T] | None],\n",
    "    result_queue: Queue[WorkResult[R]],\n",
    "    process_func: Callable[[T], R],\n",
    ") -> None:\n",
    "    \"\"\"Worker that processes items from queue.\n",
    "\n",
    "    Args:\n",
    "        worker_id: Unique worker identifier\n",
    "        work_queue: Queue to pull work from (None = shutdown signal)\n",
    "        result_queue: Queue to send results to\n",
    "        process_func: Async function to process work item data\n",
    "    \"\"\"\n",
    "    print(f\"Worker {worker_id}: Started\")\n",
    "\n",
    "    while True:\n",
    "        # Get work item (blocks until available)\n",
    "        work_item = await work_queue.get()\n",
    "\n",
    "        # Poison pill - shutdown signal\n",
    "        if work_item is None:\n",
    "            print(f\"Worker {worker_id}: Received shutdown signal\")\n",
    "            break\n",
    "\n",
    "        # Process work item\n",
    "        start_time = current_time()\n",
    "        try:\n",
    "            result = await process_func(work_item.data)\n",
    "            duration = current_time() - start_time\n",
    "\n",
    "            await result_queue.put(\n",
    "                WorkResult(\n",
    "                    work_id=work_item.id,\n",
    "                    status=ResultStatus.SUCCESS,\n",
    "                    result=result,\n",
    "                    worker_id=worker_id,\n",
    "                    duration=duration,\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            duration = current_time() - start_time\n",
    "            await result_queue.put(\n",
    "                WorkResult(\n",
    "                    work_id=work_item.id,\n",
    "                    status=ResultStatus.FAILED,\n",
    "                    error=e,\n",
    "                    worker_id=worker_id,\n",
    "                    duration=duration,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(f\"Worker {worker_id}: Shutdown complete\")\n",
    "\n",
    "\n",
    "# Test: Simple worker pool\n",
    "async def simple_process(value: int) -> int:\n",
    "    \"\"\"Simulate processing - square the value.\"\"\"\n",
    "    await sleep(0.1)  # Simulate work\n",
    "    return value * value\n",
    "\n",
    "\n",
    "async def test_basic_worker():\n",
    "    work_queue = Queue[WorkItem[int] | None].with_maxsize(10)\n",
    "    result_queue = Queue[WorkResult[int]].with_maxsize(10)\n",
    "\n",
    "    # Start worker\n",
    "    async with create_task_group() as tg:\n",
    "        tg.start_soon(worker, 1, work_queue, result_queue, simple_process)\n",
    "\n",
    "        # Add work\n",
    "        for i in range(5):\n",
    "            await work_queue.put(WorkItem(id=f\"task-{i}\", data=i))\n",
    "\n",
    "        # Send shutdown signal\n",
    "        await work_queue.put(None)\n",
    "\n",
    "    # Collect results (worker has exited, so all results are available)\n",
    "    results = []\n",
    "    try:\n",
    "        while True:\n",
    "            results.append(result_queue.get_nowait())\n",
    "    except Exception:  # WouldBlock exception when queue empty\n",
    "        pass\n",
    "\n",
    "    print(f\"\\nCollected {len(results)} results:\")\n",
    "    for r in results:\n",
    "        print(f\"  {r}\")\n",
    "\n",
    "\n",
    "await test_basic_worker()",
    "\n\n",
    "**Notes**:\n",
    "- **Blocking `get()`**: Workers block on `queue.get()` until work arrives - this is efficient (no busy-waiting)\n",
    "- **Poison Pill**: Sending `None` is the standard pattern for shutdown. Each worker consumes one poison pill.\n",
    "- **Error Handling**: Worker failures don't crash the worker - errors are captured in `WorkResult` and processing continues\n",
    "- **get_nowait()**: Non-blocking read used for result collection after workers exit (throws exception when empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add Worker Pool Management\n",
    "\n",
    "A single worker is limiting. We need a pool of N workers for parallelism. This requires coordinating worker startup, shutdown (N poison pills for N workers), and tracking when all work is complete.\n",
    "\n",
    "**Why N Poison Pills**: Each worker consumes one poison pill. If you only send one, only one worker shuts down - the rest block forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkerPoolConfig:\n",
    "    \"\"\"Configuration for worker pool.\"\"\"\n",
    "\n",
    "    num_workers: int = 4\n",
    "    work_queue_size: int = 100\n",
    "    result_queue_size: int = 100\n",
    "\n",
    "\n",
    "async def run_worker_pool(\n",
    "    work_items: list[WorkItem[T]],\n",
    "    process_func: Callable[[T], R],\n",
    "    config: WorkerPoolConfig | None = None,\n",
    ") -> list[WorkResult[R]]:\n",
    "    \"\"\"Run worker pool to process work items.\n",
    "\n",
    "    Args:\n",
    "        work_items: List of work to process\n",
    "        process_func: Async function to process each work item's data\n",
    "        config: Worker pool configuration\n",
    "\n",
    "    Returns:\n",
    "        List of work results (one per work item)\n",
    "    \"\"\"\n",
    "    config = config or WorkerPoolConfig()\n",
    "\n",
    "    # Create queues\n",
    "    work_queue = Queue[WorkItem[T] | None].with_maxsize(config.work_queue_size)\n",
    "    result_queue = Queue[WorkResult[R]].with_maxsize(config.result_queue_size)\n",
    "\n",
    "    async with create_task_group() as tg:\n",
    "        # Start workers\n",
    "        for i in range(config.num_workers):\n",
    "            tg.start_soon(worker, i, work_queue, result_queue, process_func)\n",
    "\n",
    "        # Producer: Add all work items\n",
    "        for item in work_items:\n",
    "            await work_queue.put(item)\n",
    "\n",
    "        # Send poison pills (one per worker)\n",
    "        for _ in range(config.num_workers):\n",
    "            await work_queue.put(None)\n",
    "\n",
    "    # All workers have exited - collect results\n",
    "    results = []\n",
    "    try:\n",
    "        while True:\n",
    "            results.append(result_queue.get_nowait())\n",
    "    except Exception:  # Queue empty\n",
    "        pass\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test: Process 20 items with 4 workers\n",
    "async def test_worker_pool():\n",
    "    work_items = [WorkItem(id=f\"task-{i}\", data=i) for i in range(20)]\n",
    "\n",
    "    config = WorkerPoolConfig(num_workers=4)\n",
    "    results = await run_worker_pool(work_items, simple_process, config)\n",
    "\n",
    "    print(f\"\\nProcessed {len(results)} items with {config.num_workers} workers:\")\n",
    "\n",
    "    # Group results by worker\n",
    "    worker_counts = defaultdict(int)\n",
    "    for r in results:\n",
    "        worker_counts[r.worker_id] += 1\n",
    "\n",
    "    print(\"\\nWork distribution:\")\n",
    "    for worker_id, count in sorted(worker_counts.items()):\n",
    "        print(f\"  Worker {worker_id}: {count} tasks\")\n",
    "\n",
    "    # Show sample results\n",
    "    print(\"\\nSample results:\")\n",
    "    for r in results[:5]:\n",
    "        print(f\"  {r}\")\n",
    "\n",
    "\n",
    "await test_worker_pool()",
    "\n\n",
    "**Notes**:\n",
    "- **Task Group**: Using `create_task_group()` ensures all workers complete before proceeding to result collection\n",
    "- **N Poison Pills**: Critical - you must send exactly as many poison pills as workers, or some workers hang forever\n",
    "- **Work Distribution**: Notice how work is distributed across workers (roughly equal, but not perfectly balanced)\n",
    "- **Queue Sizing**: `maxsize` provides backpressure - if producer is faster than workers, `put()` blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add Progress Tracking and Statistics\n",
    "\n",
    "Production systems need observability: how many tasks completed, failed, average processing time, per-worker throughput. We'll add comprehensive statistics collection.\n",
    "\n",
    "**Why Statistics**: Without metrics, you can't answer \"Are workers balanced?\" or \"Is worker 3 slower than others?\" Production needs visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PoolStatistics:\n",
    "    \"\"\"Statistics from worker pool execution.\"\"\"\n",
    "\n",
    "    total_items: int\n",
    "    successful: int\n",
    "    failed: int\n",
    "    total_duration: float\n",
    "    avg_task_duration: float\n",
    "    worker_counts: dict[int, int]  # Tasks per worker\n",
    "    worker_durations: dict[int, float]  # Total time per worker\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"PoolStatistics(\\n\"\n",
    "            f\"  total={self.total_items}, success={self.successful}, failed={self.failed}\\n\"\n",
    "            f\"  total_duration={self.total_duration:.3f}s, avg_task={self.avg_task_duration:.3f}s\\n\"\n",
    "            f\"  workers={len(self.worker_counts)}\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "\n",
    "async def run_worker_pool_with_stats(\n",
    "    work_items: list[WorkItem[T]],\n",
    "    process_func: Callable[[T], R],\n",
    "    config: WorkerPoolConfig | None = None,\n",
    ") -> tuple[list[WorkResult[R]], PoolStatistics]:\n",
    "    \"\"\"Run worker pool with comprehensive statistics.\n",
    "\n",
    "    Args:\n",
    "        work_items: List of work to process\n",
    "        process_func: Async function to process each work item\n",
    "        config: Worker pool configuration\n",
    "\n",
    "    Returns:\n",
    "        (results, statistics)\n",
    "    \"\"\"\n",
    "    config = config or WorkerPoolConfig()\n",
    "    start_time = current_time()\n",
    "\n",
    "    # Create queues\n",
    "    work_queue = Queue[WorkItem[T] | None].with_maxsize(config.work_queue_size)\n",
    "    result_queue = Queue[WorkResult[R]].with_maxsize(config.result_queue_size)\n",
    "\n",
    "    async with create_task_group() as tg:\n",
    "        # Start workers\n",
    "        for i in range(config.num_workers):\n",
    "            tg.start_soon(worker, i, work_queue, result_queue, process_func)\n",
    "\n",
    "        # Producer: Add all work items\n",
    "        for item in work_items:\n",
    "            await work_queue.put(item)\n",
    "\n",
    "        # Send poison pills\n",
    "        for _ in range(config.num_workers):\n",
    "            await work_queue.put(None)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    try:\n",
    "        while True:\n",
    "            results.append(result_queue.get_nowait())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_duration = current_time() - start_time\n",
    "    successful = sum(1 for r in results if r.status == ResultStatus.SUCCESS)\n",
    "    failed = sum(1 for r in results if r.status == ResultStatus.FAILED)\n",
    "\n",
    "    worker_counts = defaultdict(int)\n",
    "    worker_durations = defaultdict(float)\n",
    "    for r in results:\n",
    "        worker_counts[r.worker_id] += 1\n",
    "        worker_durations[r.worker_id] += r.duration\n",
    "\n",
    "    avg_duration = sum(r.duration for r in results) / len(results) if results else 0.0\n",
    "\n",
    "    stats = PoolStatistics(\n",
    "        total_items=len(work_items),\n",
    "        successful=successful,\n",
    "        failed=failed,\n",
    "        total_duration=total_duration,\n",
    "        avg_task_duration=avg_duration,\n",
    "        worker_counts=dict(worker_counts),\n",
    "        worker_durations=dict(worker_durations),\n",
    "    )\n",
    "\n",
    "    return results, stats\n",
    "\n",
    "\n",
    "# Test: Process with statistics\n",
    "async def variable_work(value: int) -> int:\n",
    "    \"\"\"Simulate variable processing time.\"\"\"\n",
    "    # Simulate slower processing for higher values\n",
    "    await sleep(0.05 + (value % 3) * 0.05)\n",
    "    return value * 2\n",
    "\n",
    "\n",
    "async def test_with_stats():\n",
    "    work_items = [WorkItem(id=f\"task-{i}\", data=i) for i in range(50)]\n",
    "    config = WorkerPoolConfig(num_workers=5)\n",
    "\n",
    "    results, stats = await run_worker_pool_with_stats(work_items, variable_work, config)\n",
    "\n",
    "    print(stats)\n",
    "    print(\"\\nPer-worker statistics:\")\n",
    "    for worker_id in sorted(stats.worker_counts.keys()):\n",
    "        count = stats.worker_counts[worker_id]\n",
    "        duration = stats.worker_durations[worker_id]\n",
    "        avg = duration / count if count > 0 else 0\n",
    "        print(\n",
    "            f\"  Worker {worker_id}: {count} tasks, {duration:.3f}s total, {avg:.3f}s avg\"\n",
    "        )\n",
    "\n",
    "\n",
    "await test_with_stats()",
    "\n\n",
    "**Notes**:\n",
    "- **Total vs Task Duration**: `total_duration` is wall-clock time (includes parallelism), `avg_task_duration` is per-task processing time\n",
    "- **Worker Statistics**: Per-worker metrics reveal load imbalance or slow workers (hardware issues, network problems)\n",
    "- **Production Monitoring**: Export these stats to Prometheus/DataDog for alerting and capacity planning\n",
    "- **Load Balancing**: Notice work isn't perfectly balanced - this is expected with FIFO queue (priority queues can improve this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add Result Streaming and Early Termination\n",
    "\n",
    "For long-running pools, waiting for all results before processing is inefficient. We want to stream results as they arrive (async iteration) and support early termination (stop after N results or first error).\n",
    "\n",
    "**Why Streaming**: Processing results as they arrive enables real-time updates, reduces memory usage, and allows early exit patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_worker_pool(\n",
    "    work_items: list[WorkItem[T]],\n",
    "    process_func: Callable[[T], R],\n",
    "    config: WorkerPoolConfig | None = None,\n",
    "):\n",
    "    \"\"\"Stream results from worker pool as they arrive.\n",
    "\n",
    "    Args:\n",
    "        work_items: List of work to process\n",
    "        process_func: Async function to process work items\n",
    "        config: Worker pool configuration\n",
    "\n",
    "    Yields:\n",
    "        WorkResult objects as they complete\n",
    "    \"\"\"\n",
    "    config = config or WorkerPoolConfig()\n",
    "\n",
    "    # Create queues\n",
    "    work_queue = Queue[WorkItem[T] | None].with_maxsize(config.work_queue_size)\n",
    "    result_queue = Queue[WorkResult[R] | None].with_maxsize(config.result_queue_size)\n",
    "\n",
    "    # Modified worker that signals completion\n",
    "    async def streaming_worker(\n",
    "        worker_id: int,\n",
    "        work_queue: Queue[WorkItem[T] | None],\n",
    "        result_queue: Queue[WorkResult[R] | None],\n",
    "        process_func: Callable[[T], R],\n",
    "    ) -> None:\n",
    "        while True:\n",
    "            work_item = await work_queue.get()\n",
    "            if work_item is None:\n",
    "                # Signal this worker is done\n",
    "                await result_queue.put(None)\n",
    "                break\n",
    "\n",
    "            start_time = current_time()\n",
    "            try:\n",
    "                result = await process_func(work_item.data)\n",
    "                duration = current_time() - start_time\n",
    "                await result_queue.put(\n",
    "                    WorkResult(\n",
    "                        work_id=work_item.id,\n",
    "                        status=ResultStatus.SUCCESS,\n",
    "                        result=result,\n",
    "                        worker_id=worker_id,\n",
    "                        duration=duration,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                duration = current_time() - start_time\n",
    "                await result_queue.put(\n",
    "                    WorkResult(\n",
    "                        work_id=work_item.id,\n",
    "                        status=ResultStatus.FAILED,\n",
    "                        error=e,\n",
    "                        worker_id=worker_id,\n",
    "                        duration=duration,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Start workers and producer in background\n",
    "    async with create_task_group() as tg:\n",
    "        # Start workers\n",
    "        for i in range(config.num_workers):\n",
    "            tg.start_soon(streaming_worker, i, work_queue, result_queue, process_func)\n",
    "\n",
    "        # Producer task\n",
    "        async def producer():\n",
    "            for item in work_items:\n",
    "                await work_queue.put(item)\n",
    "            # Send poison pills\n",
    "            for _ in range(config.num_workers):\n",
    "                await work_queue.put(None)\n",
    "\n",
    "        tg.start_soon(producer)\n",
    "\n",
    "        # Consumer: Stream results as they arrive\n",
    "        workers_done = 0\n",
    "        while workers_done < config.num_workers:\n",
    "            result = await result_queue.get()\n",
    "\n",
    "            if result is None:\n",
    "                workers_done += 1\n",
    "                continue\n",
    "\n",
    "            yield result\n",
    "\n",
    "\n",
    "# Test: Stream results and stop early\n",
    "async def test_streaming():\n",
    "    work_items = [WorkItem(id=f\"task-{i}\", data=i) for i in range(100)]\n",
    "    config = WorkerPoolConfig(num_workers=4)\n",
    "\n",
    "    print(\"Streaming results (first 10 only):\")\n",
    "    count = 0\n",
    "    async for result in stream_worker_pool(work_items, simple_process, config):\n",
    "        print(f\"  {result}\")\n",
    "        count += 1\n",
    "        if count >= 10:\n",
    "            print(\"  ... (stopping early, workers still running)\")\n",
    "            break\n",
    "\n",
    "\n",
    "await test_streaming()",
    "\n\n",
    "**Notes**:\n",
    "- **Worker Completion Signal**: Workers send `None` to result queue when done - the consumer counts these to know when all workers have exited\n",
    "- **Early Termination**: Breaking from the async for loop stops consuming results, but workers continue running in the background\n",
    "- **Backpressure**: If consumer stops reading, workers will block on `result_queue.put()` when queue fills (controlled by `maxsize`)\n",
    "- **Memory Efficiency**: Streaming prevents loading all results into memory - critical for large workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Working Example\n",
    "\n",
    "Here's the full production-ready implementation combining all steps. Copy-paste this into your project.\n",
    "\n",
    "**Features**:\n",
    "- \u2705 Configurable worker pool with fan-out/fan-in pattern\n",
    "- \u2705 Comprehensive statistics and progress tracking\n",
    "- \u2705 Result streaming for real-time processing\n",
    "- \u2705 Graceful shutdown with poison pill pattern\n",
    "- \u2705 Error handling and per-worker metrics\n",
    "- \u2705 Type-safe with Generic work items and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Complete production-ready fan-out/fan-in worker pool.\n",
    "\n",
    "Copy this entire cell into your project and adjust configuration.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Callable, Generic, TypeVar\n",
    "\n",
    "# lionherd-core\n",
    "from lionherd_core.libs.concurrency import Queue, create_task_group, current_time, sleep\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "R = TypeVar(\"R\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkItem(Generic[T]):\n",
    "    \"\"\"Unit of work to process.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    data: T\n",
    "    priority: int = 0\n",
    "    metadata: dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class ResultStatus(Enum):\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkResult(Generic[R]):\n",
    "    \"\"\"Result of processing work item.\"\"\"\n",
    "\n",
    "    work_id: str\n",
    "    status: ResultStatus\n",
    "    result: R | None = None\n",
    "    error: Exception | None = None\n",
    "    worker_id: int = 0\n",
    "    duration: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PoolStatistics:\n",
    "    \"\"\"Worker pool execution statistics.\"\"\"\n",
    "\n",
    "    total_items: int\n",
    "    successful: int\n",
    "    failed: int\n",
    "    total_duration: float\n",
    "    avg_task_duration: float\n",
    "    worker_counts: dict[int, int]\n",
    "    worker_durations: dict[int, float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkerPoolConfig:\n",
    "    \"\"\"Worker pool configuration.\"\"\"\n",
    "\n",
    "    num_workers: int = 4\n",
    "    work_queue_size: int = 100\n",
    "    result_queue_size: int = 100\n",
    "\n",
    "\n",
    "async def worker(\n",
    "    worker_id: int,\n",
    "    work_queue: Queue[WorkItem[T] | None],\n",
    "    result_queue: Queue[WorkResult[R]],\n",
    "    process_func: Callable[[T], R],\n",
    ") -> None:\n",
    "    \"\"\"Worker that processes items from queue.\"\"\"\n",
    "    while True:\n",
    "        work_item = await work_queue.get()\n",
    "        if work_item is None:  # Poison pill\n",
    "            break\n",
    "\n",
    "        start_time = current_time()\n",
    "        try:\n",
    "            result = await process_func(work_item.data)\n",
    "            duration = current_time() - start_time\n",
    "            await result_queue.put(\n",
    "                WorkResult(\n",
    "                    work_id=work_item.id,\n",
    "                    status=ResultStatus.SUCCESS,\n",
    "                    result=result,\n",
    "                    worker_id=worker_id,\n",
    "                    duration=duration,\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            duration = current_time() - start_time\n",
    "            await result_queue.put(\n",
    "                WorkResult(\n",
    "                    work_id=work_item.id,\n",
    "                    status=ResultStatus.FAILED,\n",
    "                    error=e,\n",
    "                    worker_id=worker_id,\n",
    "                    duration=duration,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "async def run_worker_pool(\n",
    "    work_items: list[WorkItem[T]],\n",
    "    process_func: Callable[[T], R],\n",
    "    config: WorkerPoolConfig | None = None,\n",
    ") -> tuple[list[WorkResult[R]], PoolStatistics]:\n",
    "    \"\"\"Run worker pool with fan-out/fan-in pattern.\n",
    "\n",
    "    Args:\n",
    "        work_items: List of work to process\n",
    "        process_func: Async function to process work item data\n",
    "        config: Worker pool configuration\n",
    "\n",
    "    Returns:\n",
    "        (results, statistics)\n",
    "    \"\"\"\n",
    "    config = config or WorkerPoolConfig()\n",
    "    start_time = current_time()\n",
    "\n",
    "    work_queue = Queue[WorkItem[T] | None].with_maxsize(config.work_queue_size)\n",
    "    result_queue = Queue[WorkResult[R]].with_maxsize(config.result_queue_size)\n",
    "\n",
    "    async with create_task_group() as tg:\n",
    "        # Start workers\n",
    "        for i in range(config.num_workers):\n",
    "            tg.start_soon(worker, i, work_queue, result_queue, process_func)\n",
    "\n",
    "        # Producer: add work + poison pills\n",
    "        for item in work_items:\n",
    "            await work_queue.put(item)\n",
    "        for _ in range(config.num_workers):\n",
    "            await work_queue.put(None)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    try:\n",
    "        while True:\n",
    "            results.append(result_queue.get_nowait())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Statistics\n",
    "    total_duration = current_time() - start_time\n",
    "    successful = sum(1 for r in results if r.status == ResultStatus.SUCCESS)\n",
    "    failed = sum(1 for r in results if r.status == ResultStatus.FAILED)\n",
    "\n",
    "    worker_counts = defaultdict(int)\n",
    "    worker_durations = defaultdict(float)\n",
    "    for r in results:\n",
    "        worker_counts[r.worker_id] += 1\n",
    "        worker_durations[r.worker_id] += r.duration\n",
    "\n",
    "    avg_duration = sum(r.duration for r in results) / len(results) if results else 0.0\n",
    "\n",
    "    stats = PoolStatistics(\n",
    "        total_items=len(work_items),\n",
    "        successful=successful,\n",
    "        failed=failed,\n",
    "        total_duration=total_duration,\n",
    "        avg_task_duration=avg_duration,\n",
    "        worker_counts=dict(worker_counts),\n",
    "        worker_durations=dict(worker_durations),\n",
    "    )\n",
    "\n",
    "    return results, stats\n",
    "\n",
    "\n",
    "# Example usage\n",
    "async def main():\n",
    "    \"\"\"Example: Process image batch with worker pool.\"\"\"\n",
    "\n",
    "    # Simulate image processing\n",
    "    async def process_image(image_path: str) -> dict[str, Any]:\n",
    "        await sleep(0.1)  # Simulate processing\n",
    "        return {\n",
    "            \"path\": image_path,\n",
    "            \"width\": 1920,\n",
    "            \"height\": 1080,\n",
    "            \"format\": \"JPEG\",\n",
    "        }\n",
    "\n",
    "    # Create work items\n",
    "    work_items = [\n",
    "        WorkItem(id=f\"image-{i}\", data=f\"/uploads/image_{i}.jpg\") for i in range(100)\n",
    "    ]\n",
    "\n",
    "    # Process with 8 workers\n",
    "    config = WorkerPoolConfig(num_workers=8)\n",
    "    results, stats = await run_worker_pool(work_items, process_image, config)\n",
    "\n",
    "    # Report\n",
    "    print(f\"Processed {stats.successful}/{stats.total_items} images\")\n",
    "    print(f\"Total time: {stats.total_duration:.2f}s\")\n",
    "    print(f\"Avg task: {stats.avg_task_duration:.3f}s\")\n",
    "    print(f\"\\nPer-worker breakdown:\")\n",
    "    for worker_id in sorted(stats.worker_counts.keys()):\n",
    "        count = stats.worker_counts[worker_id]\n",
    "        print(f\"  Worker {worker_id}: {count} images\")\n",
    "\n",
    "\n",
    "# Run example\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues\n",
    "\n",
    "**Worker Crash**:\n",
    "- Symptom: Individual worker throws unhandled exception\n",
    "- Fix: Wrap worker loop in try-except, log and restart\n",
    "\n",
    "**Queue Deadlock**:\n",
    "- Symptom: Producer blocks waiting for queue space, workers waiting for items\n",
    "- Fix: Ensure queue_size >= num_workers \u00d7 2, monitor queue depth\n",
    "\n",
    "**Memory Leak**:\n",
    "- Symptom: Results list grows unbounded for large batches\n",
    "- Fix: Stream results to disk or process incrementally\n",
    "\n",
    "For production patterns (monitoring, error recovery, dynamic scaling), see [lionherd-core Production Guide](https://github.com/khive-ai/lionherd-core/docs/production/worker_pools.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation: Priority-Based Work Distribution\n",
    "\n",
    "**When to Use**: Work items have different priorities (critical vs nice-to-have)\n",
    "\n",
    "**Pattern**:\n",
    "```python\n",
    "from lionherd_core.libs.concurrency import PriorityQueue\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PriorityWorkItem(Generic[T]):\n",
    "    priority: int  # Lower = higher priority\n",
    "    item: WorkItem[T] = field(compare=False)\n",
    "\n",
    "# Use PriorityQueue instead of Queue\n",
    "work_queue = PriorityQueue.with_maxsize(100)\n",
    "\n",
    "# Producer adds items with priority\n",
    "for item in work_items:\n",
    "    await work_queue.put(PriorityWorkItem(priority=item.priority, item=item))\n",
    "\n",
    "# Workers extract: priority, item = await work_queue.get()\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "- \u2705 High-priority items processed first\n",
    "- \u2705 Good for SLA-tiered workloads\n",
    "- \u274c Low-priority items may starve\n",
    "- \u274c Priority queue has ~2x overhead vs FIFO\n",
    "\n",
    "For additional variations (Dynamic Scaling, Batched Results), see [lionherd-core examples](https://github.com/khive-ai/lionherd-core/examples/worker_pool_variations.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Accomplished**:\n",
    "- \u2705 Built production-ready fan-out/fan-in worker pool using `Queue` and `create_task_group`\n",
    "- \u2705 Implemented graceful shutdown with poison pill pattern\n",
    "- \u2705 Added comprehensive statistics and per-worker metrics\n",
    "- \u2705 Learned result streaming for real-time processing\n",
    "- \u2705 Configured production monitoring and tuning parameters\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. **Fan-out/fan-in** maximizes throughput by distributing work across concurrent workers and collecting results efficiently\n",
    "2. **Poison pill pattern** (sending `None`) is the standard way to signal graceful shutdown to workers\n",
    "3. **Queue sizing matters**: `work_queue_size` controls backpressure, `result_queue_size` prevents worker blocking\n",
    "4. **Statistics are essential**: Per-worker metrics reveal load imbalance and performance issues\n",
    "5. **Streaming results** enables real-time processing and reduces memory usage for large workloads\n",
    "\n",
    "**When to Use This Pattern**:\n",
    "- \u2705 Batch processing with parallelizable tasks (image processing, data transformation, API calls)\n",
    "- \u2705 High-throughput pipelines (ETL, data ingestion, message processing)\n",
    "- \u2705 Background job processing (notification sending, report generation)\n",
    "- \u274c Tasks with complex dependencies (use DAG scheduler like Airflow instead)\n",
    "- \u274c Real-time request/response (use connection pooling or `bounded_map()` instead)\n",
    "\n",
    "## Related Resources\n",
    "\n",
    "**lionherd-core API Reference**:\n",
    "- [Queue](../../docs/api/libs/concurrency/primitives.md) - Async FIFO queue with backpressure\n",
    "- [TaskGroup](../../docs/api/libs/concurrency/task.md) - Structured concurrency for worker lifecycle\n",
    "- [PriorityQueue](../../docs/api/libs/concurrency/priority_queue.md) - Priority-based work distribution\n",
    "\n",
    "**Reference Notebooks**:\n",
    "- [Concurrency Primitives](../references/concurrency_primitives.ipynb) - Deep dive into Queue, Lock, Event\n",
    "\n",
    "**Related Tutorials**:\n",
    "- [Deadline-Aware Task Queue](./deadline_task_queue.ipynb) - Queue processing with time budgets\n",
    "- [Parallel Operations with Timeouts](./parallel_timeout.ipynb) - `bounded_map()` for simpler parallelism\n",
    "\n",
    "**External Resources**:\n",
    "- [Python asyncio Queues](https://docs.python.org/3/library/asyncio-queue.html) - Standard library queue patterns\n",
    "- [Structured Concurrency](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/) - Design principles behind TaskGroup\n",
    "- [Producer-Consumer Pattern](https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem) - Classic concurrency pattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}