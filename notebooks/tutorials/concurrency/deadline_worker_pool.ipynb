{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Deadline-Aware Worker Pool with Queue\n",
    "\n",
    "**Category**: Concurrency\n",
    "**Difficulty**: Advanced\n",
    "**Time**: 25-35 minutes\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "When building high-throughput data processing systems, you often need multiple concurrent workers processing tasks from a shared queue. Examples include API request handlers, batch data transformers, or distributed job processors. The challenge is coordinating multiple workers to process as much work as possible within a deadline while handling failures gracefully and shutting down cleanly.\n",
    "\n",
    "Consider a data pipeline that needs to process 10,000 records before a nightly ETL window closes. You can't process them sequentially (too slow), but you also can't spawn 10,000 concurrent tasks (resource exhaustion). You need a fixed-size worker pool that pulls from a queue, processes items concurrently, and stops when either the queue is empty or the deadline is reached. Workers must also handle individual task failures without crashing and shut down cleanly without leaving orphaned tasks.\n",
    "\n",
    "**Why This Matters**:\n",
    "- **Throughput**: Worker pools provide controlled parallelism - process 10-100x faster than sequential while avoiding resource exhaustion\n",
    "- **Reliability**: Individual worker failures don't stop the entire pool, and clean shutdown prevents data loss\n",
    "- **Deadline Compliance**: Production systems have SLAs - ETL windows, report generation deadlines, batch processing time limits\n",
    "\n",
    "**What You'll Build**:\n",
    "A production-ready worker pool using lionherd-core's `Queue.with_maxsize()`, `effective_deadline()`, and structured concurrency that processes tasks with multiple concurrent workers, tracks progress across workers, handles errors gracefully, and shuts down cleanly using the sentinel pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Prior Knowledge**:\n",
    "- Python async/await and task groups\n",
    "- Queue-based work distribution patterns\n",
    "- Exception handling in concurrent contexts\n",
    "- Understanding of producer-consumer patterns\n",
    "\n",
    "**Required Packages**:\n",
    "```bash\n",
    "pip install lionherd-core  # >=0.1.0\n",
    "```\n",
    "\n",
    "**Optional Reading**:\n",
    "- [API Reference: Primitives](../../docs/api/libs/concurrency/primitives.md)\n",
    "- [API Reference: Cancellation](../../docs/api/libs/concurrency/cancel.md)\n",
    "- [Reference Notebook: Concurrency Primitives](../references/concurrency_primitives.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Callable, Generic, TypeVar\n",
    "\n",
    "# lionherd-core\n",
    "from lionherd_core.libs.concurrency import (\n",
    "    Queue,\n",
    "    current_time,\n",
    "    effective_deadline,\n",
    "    fail_at,\n",
    "    move_on_at,\n",
    "    sleep,\n",
    ")\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "R = TypeVar(\"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "We'll implement a worker pool pattern using these components:\n",
    "\n",
    "1. **Queue Management**: Bounded queue with `Queue.with_maxsize()` to control memory usage\n",
    "2. **Worker Pool**: Fixed number of concurrent workers pulling from shared queue\n",
    "3. **Sentinel Pattern**: Special \"stop\" token to signal workers to shut down gracefully\n",
    "4. **Deadline Awareness**: Each worker checks `effective_deadline()` before processing tasks\n",
    "\n",
    "**Key lionherd-core Components**:\n",
    "- `Queue.with_maxsize(n)`: Bounded FIFO queue with backpressure\n",
    "- `effective_deadline()`: Query remaining time from ambient cancel scopes\n",
    "- `move_on_at(deadline)`: Silent cancellation at absolute deadline\n",
    "- `fail_at(deadline)`: Raises TimeoutError at deadline (for strict mode)\n",
    "\n",
    "**Flow**:\n",
    "```\n",
    "Producer \u2192 Queue (maxsize=100) \u2192 Worker 1 \u2192 Results\n",
    "                              \u2198 Worker 2 \u2192 Results\n",
    "                              \u2198 Worker 3 \u2192 Results\n",
    "                              \u2198 Worker N \u2192 Results\n",
    "                              \n",
    "Sentinel \u2192 Workers stop gracefully\n",
    "Deadline \u2192 All workers cancelled\n",
    "```\n",
    "\n",
    "**Expected Outcome**: Process tasks concurrently with controlled parallelism, respecting deadlines, handling errors per-worker, and shutting down cleanly without orphaned tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Quick Start: Deadline-Aware Workers in 30 Seconds\n",
    "\n",
    "from lionherd_core.libs.concurrency import Queue, effective_deadline, current_time, move_on_at, sleep\n",
    "import asyncio\n",
    "\n",
    "SENTINEL = object()  # Shutdown signal\n",
    "\n",
    "async def deadline_aware_worker(worker_id: int, queue: Queue, results: list):\n",
    "    \"\"\"Worker that checks deadline before each task.\"\"\"\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        if item is SENTINEL:\n",
    "            await queue.put(SENTINEL)\n",
    "            break\n",
    "        \n",
    "        # Check remaining time\n",
    "        deadline = effective_deadline()\n",
    "        if deadline and (deadline - current_time()) < 0.1:\n",
    "            print(f\"Worker {worker_id}: Skipping {item} (deadline near)\")\n",
    "            continue\n",
    "        \n",
    "        await sleep(0.05)\n",
    "        results.append(f\"W{worker_id}:{item}\")\n",
    "\n",
    "# Try it with 1 second deadline:\n",
    "queue = Queue.with_maxsize(20)\n",
    "results = []\n",
    "\n",
    "async with asyncio.TaskGroup() as tg:\n",
    "    deadline = current_time() + 1.0\n",
    "    \n",
    "    with move_on_at(deadline):\n",
    "        for i in range(3):\n",
    "            tg.create_task(deadline_aware_worker(i, queue, results))\n",
    "        \n",
    "        for i in range(50):\n",
    "            await queue.put(f\"task-{i}\")\n",
    "        await queue.put(SENTINEL)\n",
    "\n",
    "print(f\"Processed {len(results)}/50 tasks within deadline\")\n",
    "\n",
    "# \ud83d\udc47 Now read below for production worker pools with full error handling"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Task and Sentinel Pattern\n",
    "\n",
    "The sentinel pattern uses a special marker value to signal \"no more work\". When a worker pulls the sentinel from the queue, it knows to exit its processing loop. This enables graceful shutdown without cancellation.\n",
    "\n",
    "**Why Sentinel**: Cancellation is abrupt and can leave tasks partially processed. Sentinels let workers finish their current task and exit cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Sentinel:\n",
    "    \"\"\"Sentinel marker for queue shutdown.\"\"\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"SENTINEL\"\n",
    "\n",
    "\n",
    "SENTINEL = _Sentinel()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkItem(Generic[T]):\n",
    "    \"\"\"Item to process in worker pool.\"\"\"\n",
    "    \n",
    "    id: str\n",
    "    data: T\n",
    "    priority: int = 0\n",
    "    metadata: dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"WorkItem(id={self.id!r}, priority={self.priority})\"\n",
    "\n",
    "\n",
    "class WorkStatus(Enum):\n",
    "    \"\"\"Processing outcome.\"\"\"\n",
    "    \n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "    TIMEOUT = \"timeout\"  # Worker ran out of time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkResult(Generic[R]):\n",
    "    \"\"\"Result from processing a work item.\"\"\"\n",
    "    \n",
    "    item_id: str\n",
    "    worker_id: int\n",
    "    status: WorkStatus\n",
    "    result: R | None = None\n",
    "    error: Exception | None = None\n",
    "    duration: float = 0.0\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"WorkResult(id={self.item_id!r}, worker={self.worker_id}, status={self.status.value}, duration={self.duration:.3f}s)\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "item = WorkItem(id=\"item-001\", data={\"user_id\": 123}, priority=1)\n",
    "result = WorkResult(item_id=\"item-001\", worker_id=2, status=WorkStatus.SUCCESS, duration=0.15)\n",
    "\n",
    "print(f\"Work Item: {item}\")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Sentinel: {SENTINEL}\")",
    "\n\n",
    "**Notes**:\n",
    "- **Sentinel Pattern**: `SENTINEL` is a singleton object. Workers check `item is SENTINEL` (identity check).\n",
    "- **Worker ID**: Each result tracks which worker processed it, enabling per-worker performance analysis.\n",
    "- **Generic Types**: `WorkItem[T]` and `WorkResult[R]` support any data type and result type.\n",
    "- **Timeout Status**: Distinguishes between task failures (bad data, API errors) and deadline exhaustion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Single Worker Logic\n",
    "\n",
    "Each worker runs an infinite loop: pull item from queue, check deadline, process item, record result. The loop exits on sentinel or when deadline is reached.\n",
    "\n",
    "**Why Check Deadline**: Prevents workers from starting tasks they can't finish. If only 5 seconds remain and the task typically takes 30 seconds, skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def worker(\n",
    "    worker_id: int,\n",
    "    queue: Queue[WorkItem[T] | _Sentinel],\n",
    "    processor: Callable[[T], R],\n",
    "    results: list[WorkResult[R]],\n",
    "    min_time_remaining: float = 0.1,\n",
    ") -> None:\n",
    "    \"\"\"Single worker that processes items from queue.\n",
    "    \n",
    "    Args:\n",
    "        worker_id: Unique worker identifier\n",
    "        queue: Shared work queue\n",
    "        processor: Async function to process work item data\n",
    "        results: Shared list to append results\n",
    "        min_time_remaining: Skip task if less than this time left\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Pull next item from queue\n",
    "        item = await queue.get()\n",
    "        \n",
    "        # Check for sentinel (graceful shutdown)\n",
    "        if item is SENTINEL:\n",
    "            # Put sentinel back for other workers\n",
    "            await queue.put(SENTINEL)\n",
    "            break\n",
    "        \n",
    "        # Check remaining time\n",
    "        deadline = effective_deadline()\n",
    "        if deadline is not None:\n",
    "            remaining = deadline - current_time()\n",
    "            if remaining < min_time_remaining:\n",
    "                # Not enough time, mark as timeout\n",
    "                results.append(WorkResult(\n",
    "                    item_id=item.id,\n",
    "                    worker_id=worker_id,\n",
    "                    status=WorkStatus.TIMEOUT,\n",
    "                ))\n",
    "                # Put item back in queue for potential retry\n",
    "                await queue.put(item)\n",
    "                continue\n",
    "        \n",
    "        # Process the item\n",
    "        start = current_time()\n",
    "        try:\n",
    "            result = await processor(item.data)\n",
    "            duration = current_time() - start\n",
    "            results.append(WorkResult(\n",
    "                item_id=item.id,\n",
    "                worker_id=worker_id,\n",
    "                status=WorkStatus.SUCCESS,\n",
    "                result=result,\n",
    "                duration=duration,\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            duration = current_time() - start\n",
    "            results.append(WorkResult(\n",
    "                item_id=item.id,\n",
    "                worker_id=worker_id,\n",
    "                status=WorkStatus.FAILED,\n",
    "                error=e,\n",
    "                duration=duration,\n",
    "            ))\n",
    "\n",
    "\n",
    "# Test single worker\n",
    "async def simple_processor(data: dict) -> str:\n",
    "    \"\"\"Simulate processing.\"\"\"\n",
    "    await sleep(0.1)\n",
    "    return f\"Processed user {data['user_id']}\"\n",
    "\n",
    "\n",
    "# Create queue with 5 items\n",
    "test_queue = Queue.with_maxsize(10)\n",
    "for i in range(5):\n",
    "    await test_queue.put(WorkItem(id=f\"item-{i}\", data={\"user_id\": i}))\n",
    "await test_queue.put(SENTINEL)\n",
    "\n",
    "# Run single worker\n",
    "test_results = []\n",
    "await worker(worker_id=1, queue=test_queue, processor=simple_processor, results=test_results)\n",
    "\n",
    "print(f\"Processed {len(test_results)} items:\")\n",
    "for r in test_results:\n",
    "    print(f\"  {r}\")",
    "\n\n",
    "**Notes**:\n",
    "- **Sentinel Propagation**: When a worker sees `SENTINEL`, it puts it back in the queue so other workers can also see it and exit.\n",
    "- **Time-Aware Processing**: Workers check `effective_deadline()` before each task, not just at the start. This respects ambient cancel scopes.\n",
    "- **Item Re-queuing**: When a worker times out, it puts the item back in the queue. In production, you might want a separate \"timeout\" queue or retry limit.\n",
    "- **Shared Results**: All workers append to the same `results` list. This is safe because Python's GIL ensures list.append() is atomic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build Worker Pool Coordinator\n",
    "\n",
    "The coordinator spawns multiple workers, feeds the queue, and manages shutdown. It uses structured concurrency to ensure all workers are cleaned up properly.\n",
    "\n",
    "**Why Structured Concurrency**: If the coordinator is cancelled (deadline reached), all workers are automatically cancelled. No orphaned tasks or resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_worker_pool(\n",
    "    items: list[WorkItem[T]],\n",
    "    processor: Callable[[T], R],\n",
    "    num_workers: int = 4,\n",
    "    queue_size: int = 100,\n",
    "    deadline: float | None = None,\n",
    ") -> list[WorkResult[R]]:\n",
    "    \"\"\"Run worker pool to process items.\n",
    "    \n",
    "    Args:\n",
    "        items: Work items to process\n",
    "        processor: Async function to process each item's data\n",
    "        num_workers: Number of concurrent workers\n",
    "        queue_size: Maximum queue size (for backpressure)\n",
    "        deadline: Absolute deadline (from current_time()), None = unlimited\n",
    "    \n",
    "    Returns:\n",
    "        List of work results from all workers\n",
    "    \"\"\"\n",
    "    queue = Queue.with_maxsize(queue_size)\n",
    "    results = []\n",
    "    \n",
    "    async def producer():\n",
    "        \"\"\"Feed items into queue.\"\"\"\n",
    "        for item in items:\n",
    "            await queue.put(item)\n",
    "        # Signal workers to stop\n",
    "        await queue.put(SENTINEL)\n",
    "    \n",
    "    async def run_workers():\n",
    "        \"\"\"Spawn and run all workers.\"\"\"\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            # Spawn workers\n",
    "            for i in range(num_workers):\n",
    "                tg.create_task(worker(\n",
    "                    worker_id=i,\n",
    "                    queue=queue,\n",
    "                    processor=processor,\n",
    "                    results=results,\n",
    "                ))\n",
    "    \n",
    "    # Run with deadline if specified\n",
    "    if deadline is not None:\n",
    "        with move_on_at(deadline):\n",
    "            async with asyncio.TaskGroup() as tg:\n",
    "                tg.create_task(producer())\n",
    "                tg.create_task(run_workers())\n",
    "    else:\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            tg.create_task(producer())\n",
    "            tg.create_task(run_workers())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test worker pool with 20 items, 4 workers\n",
    "async def test_processor(data: dict) -> str:\n",
    "    await sleep(0.05)  # Simulate work\n",
    "    return f\"Result for user {data['user_id']}\"\n",
    "\n",
    "\n",
    "test_items = [WorkItem(id=f\"item-{i}\", data={\"user_id\": i}) for i in range(20)]\n",
    "\n",
    "start = current_time()\n",
    "results = await run_worker_pool(\n",
    "    items=test_items,\n",
    "    processor=test_processor,\n",
    "    num_workers=4,\n",
    "    queue_size=10,\n",
    ")\n",
    "elapsed = current_time() - start\n",
    "\n",
    "print(f\"Processed {len(results)} items in {elapsed:.2f}s with 4 workers\")\n",
    "print(f\"Expected sequential time: ~{20 * 0.05:.2f}s\")\n",
    "print(f\"Speedup: {(20 * 0.05) / elapsed:.1f}x\")\n",
    "\n",
    "# Show per-worker distribution\n",
    "from collections import Counter\n",
    "worker_counts = Counter(r.worker_id for r in results)\n",
    "print(f\"\\nWork distribution:\")\n",
    "for worker_id, count in sorted(worker_counts.items()):\n",
    "    print(f\"  Worker {worker_id}: {count} items\")",
    "\n\n",
    "**Notes**:\n",
    "- **Backpressure**: `queue_size=100` limits how many items can be queued. If the producer is faster than workers, it blocks at `queue.put()` until workers catch up.\n",
    "- **Structured Concurrency**: `TaskGroup` ensures all workers complete (or are cancelled) before returning. No orphaned tasks.\n",
    "- **Speedup**: With 4 workers and 0.05s tasks, we should see ~4x speedup. Actual speedup depends on queue overhead and task variance.\n",
    "- **Load Balancing**: Workers pull from a shared queue, so work is automatically balanced - faster workers process more items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add Comprehensive Statistics and Monitoring\n",
    "\n",
    "Production worker pools need detailed metrics: per-worker performance, success/failure rates, queue utilization, deadline compliance. This enables capacity planning and performance tuning.\n",
    "\n",
    "**Why Per-Worker Stats**: Identifies slow workers (resource contention, bad assignment), enables horizontal scaling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkerPoolStats:\n",
    "    \"\"\"Statistics from worker pool execution.\"\"\"\n",
    "    \n",
    "    total_items: int\n",
    "    successful: int\n",
    "    failed: int\n",
    "    timeout: int\n",
    "    total_duration: float\n",
    "    avg_item_duration: float\n",
    "    worker_counts: dict[int, int]  # worker_id -> item count\n",
    "    worker_avg_duration: dict[int, float]  # worker_id -> avg duration\n",
    "    deadline_reached: bool\n",
    "    throughput: float  # items per second\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"WorkerPoolStats(\\n\"\n",
    "            f\"  total={self.total_items}, success={self.successful}, \"\n",
    "            f\"failed={self.failed}, timeout={self.timeout}\\n\"\n",
    "            f\"  duration={self.total_duration:.3f}s, \"\n",
    "            f\"avg={self.avg_item_duration:.3f}s, \"\n",
    "            f\"throughput={self.throughput:.1f} items/s\\n\"\n",
    "            f\"  workers={len(self.worker_counts)}, \"\n",
    "            f\"deadline_hit={self.deadline_reached}\\n\"\n",
    "            f\")\"\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_stats(\n",
    "    results: list[WorkResult],\n",
    "    total_items: int,\n",
    "    total_duration: float,\n",
    "    deadline_reached: bool,\n",
    ") -> WorkerPoolStats:\n",
    "    \"\"\"Calculate comprehensive statistics from results.\"\"\"\n",
    "    successful = sum(1 for r in results if r.status == WorkStatus.SUCCESS)\n",
    "    failed = sum(1 for r in results if r.status == WorkStatus.FAILED)\n",
    "    timeout = sum(1 for r in results if r.status == WorkStatus.TIMEOUT)\n",
    "    \n",
    "    # Average duration (only for completed items)\n",
    "    completed = [r for r in results if r.status in (WorkStatus.SUCCESS, WorkStatus.FAILED)]\n",
    "    avg_duration = sum(r.duration for r in completed) / len(completed) if completed else 0.0\n",
    "    \n",
    "    # Per-worker stats\n",
    "    worker_counts = {}\n",
    "    worker_durations = {}\n",
    "    \n",
    "    for result in results:\n",
    "        wid = result.worker_id\n",
    "        worker_counts[wid] = worker_counts.get(wid, 0) + 1\n",
    "        \n",
    "        if result.status in (WorkStatus.SUCCESS, WorkStatus.FAILED):\n",
    "            if wid not in worker_durations:\n",
    "                worker_durations[wid] = []\n",
    "            worker_durations[wid].append(result.duration)\n",
    "    \n",
    "    worker_avg_duration = {\n",
    "        wid: sum(durations) / len(durations)\n",
    "        for wid, durations in worker_durations.items()\n",
    "    }\n",
    "    \n",
    "    throughput = len(results) / total_duration if total_duration > 0 else 0.0\n",
    "    \n",
    "    return WorkerPoolStats(\n",
    "        total_items=total_items,\n",
    "        successful=successful,\n",
    "        failed=failed,\n",
    "        timeout=timeout,\n",
    "        total_duration=total_duration,\n",
    "        avg_item_duration=avg_duration,\n",
    "        worker_counts=worker_counts,\n",
    "        worker_avg_duration=worker_avg_duration,\n",
    "        deadline_reached=deadline_reached,\n",
    "        throughput=throughput,\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_worker_pool_with_stats(\n",
    "    items: list[WorkItem[T]],\n",
    "    processor: Callable[[T], R],\n",
    "    num_workers: int = 4,\n",
    "    queue_size: int = 100,\n",
    "    deadline: float | None = None,\n",
    ") -> tuple[list[WorkResult[R]], WorkerPoolStats]:\n",
    "    \"\"\"Run worker pool with comprehensive statistics.\"\"\"\n",
    "    start_time = current_time()\n",
    "    total_items = len(items)\n",
    "    \n",
    "    results = await run_worker_pool(\n",
    "        items=items,\n",
    "        processor=processor,\n",
    "        num_workers=num_workers,\n",
    "        queue_size=queue_size,\n",
    "        deadline=deadline,\n",
    "    )\n",
    "    \n",
    "    total_duration = current_time() - start_time\n",
    "    deadline_reached = deadline is not None and current_time() >= deadline\n",
    "    \n",
    "    stats = calculate_stats(results, total_items, total_duration, deadline_reached)\n",
    "    return results, stats\n",
    "\n",
    "\n",
    "# Test with statistics\n",
    "test_items = [WorkItem(id=f\"item-{i}\", data={\"user_id\": i}) for i in range(50)]\n",
    "\n",
    "results, stats = await run_worker_pool_with_stats(\n",
    "    items=test_items,\n",
    "    processor=test_processor,\n",
    "    num_workers=5,\n",
    "    queue_size=20,\n",
    ")\n",
    "\n",
    "print(stats)\n",
    "print(\"\\nPer-worker performance:\")\n",
    "for worker_id in sorted(stats.worker_counts.keys()):\n",
    "    count = stats.worker_counts[worker_id]\n",
    "    avg_dur = stats.worker_avg_duration.get(worker_id, 0.0)\n",
    "    print(f\"  Worker {worker_id}: {count} items, avg {avg_dur:.3f}s\")",
    "\n\n",
    "**Notes**:\n",
    "- **Throughput Metric**: Items per second is the key capacity planning metric. If throughput < required rate, add more workers.\n",
    "- **Worker Balance**: Ideally, all workers process roughly the same number of items. Large variance indicates resource contention or uneven task complexity.\n",
    "- **Timeout Tracking**: Separate timeout from failures. Timeouts indicate deadline is too tight; failures indicate data quality or infrastructure issues.\n",
    "- **Average Duration**: Only includes completed tasks (success + failed). Timeout tasks have zero duration and shouldn't skew the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add Graceful Shutdown and Error Recovery\n",
    "\n",
    "Production systems need clean shutdown: finish in-flight tasks, flush results, close resources. We also need configurable error handling: retry, circuit breaker, dead letter queue.\n",
    "\n",
    "**Why Graceful Shutdown**: Abrupt cancellation can lose data (partial writes, uncommitted transactions). Clean shutdown ensures no work is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkerPoolConfig:\n",
    "    \"\"\"Configuration for worker pool.\"\"\"\n",
    "    \n",
    "    num_workers: int = 4\n",
    "    queue_size: int = 100\n",
    "    min_time_remaining: float = 0.1  # Skip task if < this time left\n",
    "    retry_failed: bool = False\n",
    "    max_retries: int = 2\n",
    "    on_error: Callable[[WorkItem, Exception], None] | None = None\n",
    "    on_timeout: Callable[[WorkItem], None] | None = None\n",
    "\n",
    "\n",
    "async def worker_with_config(\n",
    "    worker_id: int,\n",
    "    queue: Queue[WorkItem[T] | _Sentinel],\n",
    "    processor: Callable[[T], R],\n",
    "    results: list[WorkResult[R]],\n",
    "    config: WorkerPoolConfig,\n",
    "    retry_queue: Queue[WorkItem[T]] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Worker with full error handling and retry support.\"\"\"\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        \n",
    "        if item is SENTINEL:\n",
    "            await queue.put(SENTINEL)\n",
    "            break\n",
    "        \n",
    "        # Check remaining time\n",
    "        deadline = effective_deadline()\n",
    "        if deadline is not None:\n",
    "            remaining = deadline - current_time()\n",
    "            if remaining < config.min_time_remaining:\n",
    "                results.append(WorkResult(\n",
    "                    item_id=item.id,\n",
    "                    worker_id=worker_id,\n",
    "                    status=WorkStatus.TIMEOUT,\n",
    "                ))\n",
    "                if config.on_timeout:\n",
    "                    try:\n",
    "                        config.on_timeout(item)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                continue\n",
    "        \n",
    "        # Process item\n",
    "        start = current_time()\n",
    "        try:\n",
    "            result = await processor(item.data)\n",
    "            duration = current_time() - start\n",
    "            results.append(WorkResult(\n",
    "                item_id=item.id,\n",
    "                worker_id=worker_id,\n",
    "                status=WorkStatus.SUCCESS,\n",
    "                result=result,\n",
    "                duration=duration,\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            duration = current_time() - start\n",
    "            \n",
    "            # Error callback\n",
    "            if config.on_error:\n",
    "                try:\n",
    "                    config.on_error(item, e)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            # Retry logic\n",
    "            retry_count = item.metadata.get(\"retry_count\", 0)\n",
    "            if config.retry_failed and retry_count < config.max_retries and retry_queue:\n",
    "                item.metadata[\"retry_count\"] = retry_count + 1\n",
    "                item.metadata[\"last_error\"] = str(e)\n",
    "                await retry_queue.put(item)\n",
    "            else:\n",
    "                results.append(WorkResult(\n",
    "                    item_id=item.id,\n",
    "                    worker_id=worker_id,\n",
    "                    status=WorkStatus.FAILED,\n",
    "                    error=e,\n",
    "                    duration=duration,\n",
    "                ))\n",
    "\n",
    "\n",
    "async def run_worker_pool_production(\n",
    "    items: list[WorkItem[T]],\n",
    "    processor: Callable[[T], R],\n",
    "    config: WorkerPoolConfig,\n",
    "    deadline: float | None = None,\n",
    ") -> tuple[list[WorkResult[R]], WorkerPoolStats]:\n",
    "    \"\"\"Production-ready worker pool with full error handling.\"\"\"\n",
    "    start_time = current_time()\n",
    "    total_items = len(items)\n",
    "    \n",
    "    work_queue = Queue.with_maxsize(config.queue_size)\n",
    "    retry_queue = Queue.with_maxsize(config.queue_size) if config.retry_failed else None\n",
    "    results = []\n",
    "    \n",
    "    async def producer():\n",
    "        \"\"\"Feed items into queue.\"\"\"\n",
    "        for item in items:\n",
    "            await work_queue.put(item)\n",
    "        \n",
    "        # Process retries if enabled\n",
    "        if retry_queue:\n",
    "            while True:\n",
    "                try:\n",
    "                    retry_item = retry_queue.get_nowait()\n",
    "                    await work_queue.put(retry_item)\n",
    "                except Exception:\n",
    "                    break\n",
    "        \n",
    "        await work_queue.put(SENTINEL)\n",
    "    \n",
    "    async def run_workers():\n",
    "        \"\"\"Spawn and run all workers.\"\"\"\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            for i in range(config.num_workers):\n",
    "                tg.create_task(worker_with_config(\n",
    "                    worker_id=i,\n",
    "                    queue=work_queue,\n",
    "                    processor=processor,\n",
    "                    results=results,\n",
    "                    config=config,\n",
    "                    retry_queue=retry_queue,\n",
    "                ))\n",
    "    \n",
    "    # Run with deadline if specified\n",
    "    if deadline is not None:\n",
    "        with move_on_at(deadline):\n",
    "            async with asyncio.TaskGroup() as tg:\n",
    "                tg.create_task(producer())\n",
    "                tg.create_task(run_workers())\n",
    "    else:\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            tg.create_task(producer())\n",
    "            tg.create_task(run_workers())\n",
    "    \n",
    "    total_duration = current_time() - start_time\n",
    "    deadline_reached = deadline is not None and current_time() >= deadline\n",
    "    \n",
    "    stats = calculate_stats(results, total_items, total_duration, deadline_reached)\n",
    "    return results, stats\n",
    "\n",
    "\n",
    "# Test with errors and deadline\n",
    "async def flaky_processor(data: dict) -> str:\n",
    "    \"\"\"Processor that fails on some items.\"\"\"\n",
    "    await sleep(0.05)\n",
    "    if data[\"user_id\"] % 7 == 0:\n",
    "        raise ValueError(f\"Validation error for user {data['user_id']}\")\n",
    "    return f\"Processed user {data['user_id']}\"\n",
    "\n",
    "\n",
    "def log_error(item: WorkItem, error: Exception):\n",
    "    print(f\"Error processing {item.id}: {error}\")\n",
    "\n",
    "\n",
    "test_items = [WorkItem(id=f\"item-{i}\", data={\"user_id\": i}) for i in range(30)]\n",
    "\n",
    "config = WorkerPoolConfig(\n",
    "    num_workers=4,\n",
    "    queue_size=15,\n",
    "    retry_failed=True,\n",
    "    max_retries=1,\n",
    "    on_error=log_error,\n",
    ")\n",
    "\n",
    "deadline = current_time() + 2.0  # 2 second deadline\n",
    "results, stats = await run_worker_pool_production(\n",
    "    items=test_items,\n",
    "    processor=flaky_processor,\n",
    "    config=config,\n",
    "    deadline=deadline,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Statistics:\")\n",
    "print(stats)",
    "\n\n",
    "**Notes**:\n",
    "- **Retry Queue**: Failed items are put in a separate retry queue, preventing retry loops from blocking new work.\n",
    "- **Callbacks Don't Throw**: Error/timeout callbacks are wrapped in try-except. Callback failures must not break the worker.\n",
    "- **Graceful Shutdown**: Workers finish their current task before exiting. Sentinel propagation ensures all workers stop.\n",
    "- **Retry Metadata**: Track retry count and last error in item metadata for debugging and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Working Example\n",
    "\n",
    "Here's the full production-ready implementation combining all steps. Copy-paste this into your project.\n",
    "\n",
    "**Features**:\n",
    "- \u2705 Worker pool with configurable concurrency\n",
    "- \u2705 Queue-based work distribution with backpressure\n",
    "- \u2705 Sentinel pattern for graceful shutdown\n",
    "- \u2705 Deadline awareness with `effective_deadline()`\n",
    "- \u2705 Comprehensive statistics and monitoring\n",
    "- \u2705 Error handling with retry support\n",
    "- \u2705 Configurable callbacks for observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Complete production-ready worker pool implementation.\n",
    "\n",
    "Copy this entire cell into your project and adjust configuration.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Callable, Generic, TypeVar\n",
    "\n",
    "# lionherd-core\n",
    "from lionherd_core.libs.concurrency import (\n",
    "    Queue,\n",
    "    current_time,\n",
    "    effective_deadline,\n",
    "    move_on_at,\n",
    "    sleep,\n",
    ")\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "R = TypeVar(\"R\")\n",
    "\n",
    "\n",
    "class _Sentinel:\n",
    "    \"\"\"Sentinel marker for graceful shutdown.\"\"\"\n",
    "    def __repr__(self) -> str:\n",
    "        return \"SENTINEL\"\n",
    "\n",
    "SENTINEL = _Sentinel()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkItem(Generic[T]):\n",
    "    \"\"\"Item to process.\"\"\"\n",
    "    id: str\n",
    "    data: T\n",
    "    priority: int = 0\n",
    "    metadata: dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class WorkStatus(Enum):\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "    TIMEOUT = \"timeout\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkResult(Generic[R]):\n",
    "    \"\"\"Processing result.\"\"\"\n",
    "    item_id: str\n",
    "    worker_id: int\n",
    "    status: WorkStatus\n",
    "    result: R | None = None\n",
    "    error: Exception | None = None\n",
    "    duration: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkerPoolStats:\n",
    "    \"\"\"Pool statistics.\"\"\"\n",
    "    total_items: int\n",
    "    successful: int\n",
    "    failed: int\n",
    "    timeout: int\n",
    "    total_duration: float\n",
    "    avg_item_duration: float\n",
    "    worker_counts: dict[int, int]\n",
    "    worker_avg_duration: dict[int, float]\n",
    "    deadline_reached: bool\n",
    "    throughput: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkerPoolConfig:\n",
    "    \"\"\"Pool configuration.\"\"\"\n",
    "    num_workers: int = 4\n",
    "    queue_size: int = 100\n",
    "    min_time_remaining: float = 0.1\n",
    "    retry_failed: bool = False\n",
    "    max_retries: int = 2\n",
    "    on_error: Callable[[WorkItem, Exception], None] | None = None\n",
    "    on_timeout: Callable[[WorkItem], None] | None = None\n",
    "\n",
    "\n",
    "async def worker(\n",
    "    worker_id: int,\n",
    "    queue: Queue[WorkItem[T] | _Sentinel],\n",
    "    processor: Callable[[T], R],\n",
    "    results: list[WorkResult[R]],\n",
    "    config: WorkerPoolConfig,\n",
    "    retry_queue: Queue[WorkItem[T]] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Worker loop.\"\"\"\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        \n",
    "        if item is SENTINEL:\n",
    "            await queue.put(SENTINEL)\n",
    "            break\n",
    "        \n",
    "        # Check deadline\n",
    "        deadline = effective_deadline()\n",
    "        if deadline is not None:\n",
    "            remaining = deadline - current_time()\n",
    "            if remaining < config.min_time_remaining:\n",
    "                results.append(WorkResult(\n",
    "                    item_id=item.id,\n",
    "                    worker_id=worker_id,\n",
    "                    status=WorkStatus.TIMEOUT,\n",
    "                ))\n",
    "                if config.on_timeout:\n",
    "                    try:\n",
    "                        config.on_timeout(item)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                continue\n",
    "        \n",
    "        # Process\n",
    "        start = current_time()\n",
    "        try:\n",
    "            result = await processor(item.data)\n",
    "            duration = current_time() - start\n",
    "            results.append(WorkResult(\n",
    "                item_id=item.id,\n",
    "                worker_id=worker_id,\n",
    "                status=WorkStatus.SUCCESS,\n",
    "                result=result,\n",
    "                duration=duration,\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            duration = current_time() - start\n",
    "            \n",
    "            if config.on_error:\n",
    "                try:\n",
    "                    config.on_error(item, e)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            retry_count = item.metadata.get(\"retry_count\", 0)\n",
    "            if config.retry_failed and retry_count < config.max_retries and retry_queue:\n",
    "                item.metadata[\"retry_count\"] = retry_count + 1\n",
    "                await retry_queue.put(item)\n",
    "            else:\n",
    "                results.append(WorkResult(\n",
    "                    item_id=item.id,\n",
    "                    worker_id=worker_id,\n",
    "                    status=WorkStatus.FAILED,\n",
    "                    error=e,\n",
    "                    duration=duration,\n",
    "                ))\n",
    "\n",
    "\n",
    "async def run_worker_pool(\n",
    "    items: list[WorkItem[T]],\n",
    "    processor: Callable[[T], R],\n",
    "    config: WorkerPoolConfig | None = None,\n",
    "    deadline: float | None = None,\n",
    ") -> tuple[list[WorkResult[R]], WorkerPoolStats]:\n",
    "    \"\"\"Run worker pool with deadline awareness.\n",
    "    \n",
    "    Args:\n",
    "        items: Work items to process\n",
    "        processor: Async function to process item data\n",
    "        config: Pool configuration\n",
    "        deadline: Absolute deadline (from current_time())\n",
    "    \n",
    "    Returns:\n",
    "        (results, statistics)\n",
    "    \"\"\"\n",
    "    config = config or WorkerPoolConfig()\n",
    "    start_time = current_time()\n",
    "    total_items = len(items)\n",
    "    \n",
    "    work_queue = Queue.with_maxsize(config.queue_size)\n",
    "    retry_queue = Queue.with_maxsize(config.queue_size) if config.retry_failed else None\n",
    "    results = []\n",
    "    \n",
    "    async def producer():\n",
    "        for item in items:\n",
    "            await work_queue.put(item)\n",
    "        if retry_queue:\n",
    "            while True:\n",
    "                try:\n",
    "                    retry_item = retry_queue.get_nowait()\n",
    "                    await work_queue.put(retry_item)\n",
    "                except Exception:\n",
    "                    break\n",
    "        await work_queue.put(SENTINEL)\n",
    "    \n",
    "    async def run_workers():\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            for i in range(config.num_workers):\n",
    "                tg.create_task(worker(\n",
    "                    worker_id=i,\n",
    "                    queue=work_queue,\n",
    "                    processor=processor,\n",
    "                    results=results,\n",
    "                    config=config,\n",
    "                    retry_queue=retry_queue,\n",
    "                ))\n",
    "    \n",
    "    if deadline is not None:\n",
    "        with move_on_at(deadline):\n",
    "            async with asyncio.TaskGroup() as tg:\n",
    "                tg.create_task(producer())\n",
    "                tg.create_task(run_workers())\n",
    "    else:\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            tg.create_task(producer())\n",
    "            tg.create_task(run_workers())\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_duration = current_time() - start_time\n",
    "    deadline_reached = deadline is not None and current_time() >= deadline\n",
    "    \n",
    "    successful = sum(1 for r in results if r.status == WorkStatus.SUCCESS)\n",
    "    failed = sum(1 for r in results if r.status == WorkStatus.FAILED)\n",
    "    timeout = sum(1 for r in results if r.status == WorkStatus.TIMEOUT)\n",
    "    \n",
    "    completed = [r for r in results if r.status in (WorkStatus.SUCCESS, WorkStatus.FAILED)]\n",
    "    avg_duration = sum(r.duration for r in completed) / len(completed) if completed else 0.0\n",
    "    \n",
    "    worker_counts = {}\n",
    "    worker_durations = {}\n",
    "    for result in results:\n",
    "        wid = result.worker_id\n",
    "        worker_counts[wid] = worker_counts.get(wid, 0) + 1\n",
    "        if result.status in (WorkStatus.SUCCESS, WorkStatus.FAILED):\n",
    "            if wid not in worker_durations:\n",
    "                worker_durations[wid] = []\n",
    "            worker_durations[wid].append(result.duration)\n",
    "    \n",
    "    worker_avg_duration = {\n",
    "        wid: sum(durations) / len(durations)\n",
    "        for wid, durations in worker_durations.items()\n",
    "    }\n",
    "    \n",
    "    throughput = len(results) / total_duration if total_duration > 0 else 0.0\n",
    "    \n",
    "    stats = WorkerPoolStats(\n",
    "        total_items=total_items,\n",
    "        successful=successful,\n",
    "        failed=failed,\n",
    "        timeout=timeout,\n",
    "        total_duration=total_duration,\n",
    "        avg_item_duration=avg_duration,\n",
    "        worker_counts=worker_counts,\n",
    "        worker_avg_duration=worker_avg_duration,\n",
    "        deadline_reached=deadline_reached,\n",
    "        throughput=throughput,\n",
    "    )\n",
    "    \n",
    "    return results, stats\n",
    "\n",
    "\n",
    "# Example usage\n",
    "async def main():\n",
    "    \"\"\"Example: Process user notifications with worker pool.\"\"\"\n",
    "    \n",
    "    # Simulate notification sending\n",
    "    async def send_notification(data: dict) -> str:\n",
    "        await sleep(0.1)  # Simulate API call\n",
    "        return f\"Sent to user {data['user_id']}\"\n",
    "    \n",
    "    # Create work items\n",
    "    items = [\n",
    "        WorkItem(id=f\"notify-{i}\", data={\"user_id\": i})\n",
    "        for i in range(100)\n",
    "    ]\n",
    "    \n",
    "    # Configure pool\n",
    "    config = WorkerPoolConfig(\n",
    "        num_workers=10,\n",
    "        queue_size=50,\n",
    "        retry_failed=True,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    \n",
    "    # Run with 5 second deadline\n",
    "    deadline = current_time() + 5.0\n",
    "    results, stats = await run_worker_pool(\n",
    "        items=items,\n",
    "        processor=send_notification,\n",
    "        config=config,\n",
    "        deadline=deadline,\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Processed {stats.successful}/{stats.total_items} notifications\")\n",
    "    print(f\"Failed: {stats.failed}, Timeout: {stats.timeout}\")\n",
    "    print(f\"Throughput: {stats.throughput:.1f} items/s\")\n",
    "    print(f\"Total time: {stats.total_duration:.2f}s\")\n",
    "    print(f\"Deadline reached: {stats.deadline_reached}\")\n",
    "\n",
    "\n",
    "# Run the example\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues\n",
    "\n",
    "**Sentinel Lost**:\n",
    "- Symptom: Sentinel consumed but not re-queued, workers never exit\n",
    "- Fix: Always re-queue sentinel after consuming: `await queue.put(SENTINEL)`\n",
    "\n",
    "**Tasks Started Too Late**:\n",
    "- Symptom: Workers start tasks with insufficient time to complete\n",
    "- Fix: Check `effective_deadline() - current_time() >= min_time_remaining`\n",
    "\n",
    "**Queue Backpressure Issues**:\n",
    "- Symptom: Producer blocks when queue fills, workers idle when empty\n",
    "- Fix: Set queue_size = 2-5 \u00d7 num_workers for balanced buffering\n",
    "\n",
    "For production patterns (retry logic, monitoring, graceful shutdown), see [lionherd-core Production Guide](https://github.com/khive-ai/lionherd-core/docs/production/deadline_workers.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation: Priority Worker Pool\n",
    "\n",
    "**When to Use**: Work items have different priorities (critical vs nice-to-have)\n",
    "\n",
    "**Pattern**:\n",
    "```python\n",
    "from lionherd_core.libs.concurrency import PriorityQueue\n",
    "\n",
    "async def run_priority_worker_pool(\n",
    "    items: list[WorkItem[T]],\n",
    "    processor: Callable[[T], R],\n",
    "    config: WorkerPoolConfig,\n",
    ") -> tuple[list[WorkResult[R]], WorkerPoolStats]:\n",
    "    # Use PriorityQueue instead of regular Queue\n",
    "    work_queue = PriorityQueue.with_maxsize(config.queue_size)\n",
    "    \n",
    "    # Producer puts items with priority\n",
    "    for item in items:\n",
    "        # Higher priority = processed first (negate for max-heap)\n",
    "        await work_queue.put((-item.priority, item))\n",
    "    \n",
    "    # Workers extract: priority, item = await work_queue.get()\n",
    "    # ... rest similar to base implementation\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "- \u2705 High-priority items processed first (better SLA compliance)\n",
    "- \u2705 Good for mixed workloads (user-facing + background)\n",
    "- \u274c Low-priority items may starve if high-priority keeps arriving\n",
    "- \u274c Priority queue ~2x overhead vs FIFO queue\n",
    "\n",
    "For additional variations (Dynamic Scaling, Two-Phase Workers), see [lionherd-core examples](https://github.com/khive-ai/lionherd-core/examples/deadline_worker_variations.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Accomplished**:\n",
    "- \u2705 Built production-ready worker pool with `Queue.with_maxsize()` and structured concurrency\n",
    "- \u2705 Implemented sentinel pattern for graceful shutdown without abrupt cancellation\n",
    "- \u2705 Added deadline awareness using `effective_deadline()` to stop processing at deadline\n",
    "- \u2705 Created comprehensive statistics tracking per-worker performance and throughput\n",
    "- \u2705 Implemented error handling with retry logic and configurable callbacks\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. **Sentinel pattern** enables graceful shutdown - workers finish current task and exit cleanly without cancellation\n",
    "2. **Bounded queues** provide backpressure - prevent memory exhaustion when producer is faster than workers\n",
    "3. **`effective_deadline()`** respects ambient cancel scopes - workers check remaining time before each task\n",
    "4. **Per-worker statistics** identify performance bottlenecks - uneven work distribution indicates resource contention\n",
    "\n",
    "**When to Use This Pattern**:\n",
    "- \u2705 High-throughput batch processing (ETL, data transformations, API batch operations)\n",
    "- \u2705 Deadline-bound workloads (nightly jobs, report generation, maintenance windows)\n",
    "- \u2705 I/O-bound tasks where parallelism >> sequential (network calls, database queries)\n",
    "- \u274c CPU-bound tasks on single-core systems (limited by GIL, use multiprocessing instead)\n",
    "- \u274c Tasks with complex dependencies (use DAG scheduler instead of queue)\n",
    "\n",
    "## Related Resources\n",
    "\n",
    "**lionherd-core API Reference**:\n",
    "- [Primitives](../../docs/api/libs/concurrency/primitives.md) - `Queue`, `CapacityLimiter`, `Semaphore`\n",
    "- [Cancellation Utilities](../../docs/api/libs/concurrency/cancel.md) - `effective_deadline()`, `fail_at()`, `move_on_at()`\n",
    "- [Concurrency Patterns](../../docs/api/libs/concurrency/patterns.md) - `bounded_map()`, `gather()`, `CompletionStream`\n",
    "\n",
    "**Reference Notebooks**:\n",
    "- [Concurrency Primitives](../references/concurrency_primitives.ipynb) - Queue, Event, Lock patterns\n",
    "- [Cancellation Patterns](../references/concurrency_cancel.ipynb) - Deadline and timeout management\n",
    "\n",
    "**Related Tutorials**:\n",
    "- [Deadline-Aware Task Queue](./deadline_task_queue.ipynb) - Sequential processing with deadlines\n",
    "- [Parallel Operations with Timeouts](./parallel_timeouts.ipynb) - `bounded_map()` with deadline awareness\n",
    "\n",
    "**External Resources**:\n",
    "- [Structured Concurrency](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/) - Design principles behind worker pools\n",
    "- [Python asyncio TaskGroup](https://docs.python.org/3/library/asyncio-task.html#task-groups) - Structured concurrency primitives\n",
    "- [AWS SQS Best Practices](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html) - Real-world queue-based processing patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}